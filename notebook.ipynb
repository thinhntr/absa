{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gtxqTCE_Prat"
      ],
      "toc_visible": true
    },
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "89096161-8db2-4c83-bd09-f2de34fe0e65",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 2
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH8Zl_K6X_io"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwKIN6EZ-eh0"
      },
      "source": [
        "!pip install -Uq emoji \\\n",
        "                 optuna \\\n",
        "                 flashtext \\\n",
        "                 underthesea \\\n",
        "                 scikit-learn \\"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00001-73dc905a-f8e8-4f5a-b542-50a626b67551",
        "deepnote_cell_type": "markdown",
        "id": "NZ2MidPrPrae",
        "tags": []
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0H8h_UD9PBW"
      },
      "source": [
        "## TextCleaner class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-c74a2f8e-bc89-467a-85c2-abee6f93cd4f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1175,
        "execution_start": 1636288345626,
        "id": "ipXDgXJ5Pray",
        "source_hash": "e9c1ba90",
        "tags": []
      },
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "from emoji import get_emoji_regexp\n",
        "from flashtext import KeywordProcessor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "HASHTAG = 'hashtag'\n",
        "\n",
        "class TextCleanerBase(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Find emojis\n",
        "        emoji = get_emoji_regexp()\n",
        "\n",
        "        # Create preprocessing function\n",
        "        self.remove_emoji      = partial(emoji.sub, '')\n",
        "        self.normalize_unicode = partial(unicodedata.normalize, 'NFC')\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not isinstance(X, pd.Series):\n",
        "            X = pd.Series(X)\n",
        "\n",
        "        return X.apply(str.lower) \\\n",
        "                .apply(self.remove_emoji) \\\n",
        "                .apply(self.normalize_unicode)\n",
        "        \n",
        "\n",
        "class TextCleaner(TextCleanerBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Find hashtag\n",
        "        hashtag = re.compile('#\\S+')\n",
        "\n",
        "        # Find price tags\n",
        "        pricetag = '((?:(?:\\d+[,\\.]?)+) ?(?:nghìn đồng|đồng|k|vnd|d|đ))'\n",
        "        pricetag = re.compile(pricetag)\n",
        "\n",
        "        # Find special characters\n",
        "        specialchar = r\"[\\\"#$%&'()*+,\\-.\\/\\\\:;<=>@[\\]^_`{|}~\\n\\r\\t]\"\n",
        "        specialchar = re.compile(specialchar)\n",
        "\n",
        "        # Spelling correction\n",
        "        rules = {\n",
        "            \"òa\":[\"oà\"], \"óa\":[\"oá\"], \"ỏa\":[\"oả\"], \"õa\":[\"oã\"], \"ọa\":[\"oạ\"],\n",
        "            \"òe\":[\"oè\"], \"óe\":[\"oé\"], \"ỏe\":[\"oẻ\"], \"õe\":[\"oẽ\"], \"ọe\":[\"oẹ\"],\n",
        "            \"ùy\":[\"uỳ\"], \"úy\":[\"uý\"], \"ủy\":[\"uỷ\"], \"ũy\":[\"uỹ\"], \"ụy\":[\"uỵ\"],\n",
        "            \"ùa\":[\"uà\"], \"úa\":[\"uá\"], \"ủa\":[\"uả\"], \"ũa\":[\"uã\"], \"ụa\":[\"uạ\"],\n",
        "            \"xảy\":[\"xẩy\"], \"bảy\":[\"bẩy\"], \"gãy\":[\"gẫy\"],\n",
        "            \"không\":[\"k\", \"hông\", \"ko\", \"khong\"]}\n",
        "\n",
        "        kp = KeywordProcessor(case_sensitive=False)\n",
        "        kp.add_keywords_from_dict(rules)\n",
        "\n",
        "        # Create preprocessing functions\n",
        "        self.autocorrect          = kp.replace_keywords\n",
        "        self.normalize_pricetag   = partial(pricetag.sub, 'giá_tiền')\n",
        "        self.normalize_hashtag    = partial(hashtag.sub, HASHTAG)\n",
        "        self.remove_specialchar   = partial(specialchar.sub, '')\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = super().transform(X)\n",
        "\n",
        "        return X.apply(self.autocorrect) \\\n",
        "                .apply(self.normalize_pricetag) \\\n",
        "                .apply(self.normalize_hashtag) \\\n",
        "                .apply(self.remove_specialchar)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM0ohCtvRaHZ"
      },
      "source": [
        "## mo2ml - Multioutput to multilabel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZif1GwORnKq"
      },
      "source": [
        "aspects = ['FOOD#PRICES',\n",
        "           'FOOD#QUALITY',\n",
        "           'FOOD#STYLE&OPTIONS',\n",
        "           'DRINKS#PRICES',\n",
        "           'DRINKS#QUALITY',\n",
        "           'DRINKS#STYLE&OPTIONS',\n",
        "           'RESTAURANT#PRICES',\n",
        "           'RESTAURANT#GENERAL',\n",
        "           'RESTAURANT#MISCELLANEOUS',\n",
        "           'SERVICE#GENERAL',\n",
        "           'AMBIENCE#GENERAL',\n",
        "           'LOCATION#GENERAL']\n",
        "\n",
        "sentiments = ['-', 'o', '+']\n",
        "\n",
        "def mo2ml(y):\n",
        "    \"\"\"Convert multi-output to multi-label data\n",
        "    \"\"\"\n",
        "    newcols = [f'{a} {s}' for a in aspects for s in sentiments]\n",
        "\n",
        "    nrows, ncols = len(y), len(newcols)\n",
        "    ml = pd.DataFrame(np.zeros((nrows, ncols), dtype='bool'),\n",
        "                      columns=newcols)\n",
        "    \n",
        "    for i, a in enumerate(aspects):\n",
        "        for j in range(1, 4):\n",
        "            indices = y[a] == j\n",
        "            ml.iloc[indices, i * 3 + j - 1] = True\n",
        "\n",
        "    return ml"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhbl0ne_6uET"
      },
      "source": [
        "## mo2df - Multioutput to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGc_-Ed_6zKL"
      },
      "source": [
        "def mo2df(y):\n",
        "    if isinstance(y, pd.DataFrame):\n",
        "        return y\n",
        "    return pd.DataFrame(y, columns=aspects)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVRMy8DDNcGz"
      },
      "source": [
        "## Download csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00000-d1f2d812-aabe-4dfd-9477-93be642302e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 304,
        "execution_start": 1636288255576,
        "id": "gZj_jnCGPran",
        "source_hash": "1fdd19ce",
        "tags": [],
        "outputId": "c12b00ef-31a4-42f1-cfa2-57675acb8639"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "root = 'https://raw.githubusercontent.com/thinhntr/absa/main/data/csv/'\n",
        "train_url = root + 'train.csv'\n",
        "dev_url = root + 'dev.csv'\n",
        "test_url = root + 'test.csv'\n",
        "\n",
        "def read_csv(url):\n",
        "    df = pd.read_csv(url)\n",
        "\n",
        "    X = df.pop('review')\n",
        "    y = df.replace({np.nan: 0, \n",
        "                    'negative': 1, \n",
        "                    'neutral': 2, \n",
        "                    'positive': 3}).astype(np.uint8)\n",
        "\n",
        "    print('X.shape:', X.shape, 'y.shape:', y.shape)\n",
        "    return X, y\n",
        "\n",
        "Xtrain, ytrain = read_csv(train_url)\n",
        "Xdev,   ydev   = read_csv(dev_url)\n",
        "Xtest,  ytest  = read_csv(test_url)\n",
        "\n",
        "# Basic text cleanup\n",
        "cleaner_base  = TextCleanerBase()\n",
        "\n",
        "xtrain_basecl = cleaner_base.transform(Xtrain)\n",
        "xdev_basecl   = cleaner_base.transform(Xdev)\n",
        "xtest_basecl  = cleaner_base.transform(Xtest)\n",
        "\n",
        "# Advanced text cleanup\n",
        "cleaner       = TextCleaner()\n",
        "\n",
        "xtrain        = cleaner.transform(Xtrain)\n",
        "xdev          = cleaner.transform(Xdev)\n",
        "xtest         = cleaner.transform(Xtest)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (2961,) y.shape: (2961, 12)\n",
            "X.shape: (1290,) y.shape: (1290, 12)\n",
            "X.shape: (500,) y.shape: (500, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y target for phase a\n",
        "ytrain_a  = ytrain != 0\n",
        "ydev_a    = ydev   != 0\n",
        "ytest_a   = ytest  != 0\n",
        "\n",
        "# y target for phase b\n",
        "ytrain_b  = ytrain.copy()\n",
        "ydev_b    = ydev  .copy()\n",
        "ytest_b   = ytest .copy()\n",
        "\n",
        "# y target for evaluation\n",
        "ytrain_ml = mo2ml(ytrain)\n",
        "ydev_ml   = mo2ml(ydev)\n",
        "ytest_ml  = mo2ml(ytest)"
      ],
      "metadata": {
        "id": "MeYOZp0Uc2I9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRjGeJbe89ww"
      },
      "source": [
        "# import requests\n",
        "\n",
        "# def download_img(url, save_path):\n",
        "#     with open(save_path, 'wb') as f:\n",
        "#         response = requests.get(url)\n",
        "#         f.write(response.content)\n",
        "\n",
        "# download_img('https://image.flaticon.com/icons/png/512/24/24208.png', 'vn.png')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00003-b0c4c0b2-689b-4f0b-9bb7-f78acf400fba",
        "deepnote_cell_type": "markdown",
        "id": "gtxqTCE_Prat",
        "tags": []
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y6zihw1FZHx"
      },
      "source": [
        "## References\n",
        "- https://developers.google.com/machine-learning/guides/text-classification/step-2\n",
        "- https://github.com/google/eng-edu/blob/main/ml/guides/text_classification/explore_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AJu2t0rK71J"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QuSTxwCINC9"
      },
      "source": [
        "def get_num_words_per_sample(sample_texts):\n",
        "    \"\"\"Gets the median number of words per sample given corpus.\n",
        "\n",
        "    # Arguments\n",
        "        sample_texts: list, sample texts.\n",
        "\n",
        "    # Returns\n",
        "        int, median number of words per sample.\n",
        "    \"\"\"\n",
        "    num_words = [len(s.split()) for s in sample_texts]\n",
        "    return np.median(num_words)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxP6y3tQK_IF"
      },
      "source": [
        "## Key metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_xhI3z6Gw1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb7e105-a809-4628-ca64-6f3adddb54cf"
      },
      "source": [
        "num_samples = len(xtrain)\n",
        "num_aspects = len(aspects)\n",
        "num_classes = num_aspects * 3\n",
        "num_words_per_sample = get_num_words_per_sample(xtrain)\n",
        "sw_ratio = num_samples / num_words_per_sample\n",
        "\n",
        "\n",
        "print(\"Xtrain key metrics\")\n",
        "print(\"Number of samples:\", num_samples)\n",
        "print(\"Number of aspects:\", num_aspects)\n",
        "print(\"Number of classes:\", num_classes)\n",
        "print(\"Number of words per sample:\", num_words_per_sample)\n",
        "print(\"Number of samples/number of words per sample ratio\", sw_ratio)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xtrain key metrics\n",
            "Number of samples: 2961\n",
            "Number of aspects: 12\n",
            "Number of classes: 36\n",
            "Number of words per sample: 49.0\n",
            "Number of samples/number of words per sample ratio 60.42857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03IBx4ZS8tn4"
      },
      "source": [
        "## Class distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohAmNN9b50AH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f8ec9128-b896-43ca-ea9d-62d7dc2aba5e"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def get_pie(df, name):\n",
        "    count = df.sum(axis=0)\n",
        "    return go.Pie(labels=count.index, values=count, \n",
        "                  textposition='inside', name=name)\n",
        "\n",
        "\n",
        "names = ('Train', 'Dev', 'Test')\n",
        "phaseA = (ytrain_a, ydev_a, ytest_a)\n",
        "\n",
        "fig = make_subplots(cols=3, subplot_titles=names,\n",
        "                    specs=[[{'type': 'pie'}] * 3])\n",
        "\n",
        "for i, (df, name) in enumerate(zip(phaseA, names), 1):\n",
        "    fig.add_trace(get_pie(df, name), row=1, col=i)\n",
        "\n",
        "fig.update_layout(title='# of samples per aspect')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"dfc2e6a9-84dc-484d-a6ae-803f9191075b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"dfc2e6a9-84dc-484d-a6ae-803f9191075b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'dfc2e6a9-84dc-484d-a6ae-803f9191075b',\n",
              "                        [{\"domain\": {\"x\": [0.0, 0.2888888888888889], \"y\": [0.0, 1.0]}, \"labels\": [\"FOOD#PRICES\", \"FOOD#QUALITY\", \"FOOD#STYLE&OPTIONS\", \"DRINKS#PRICES\", \"DRINKS#QUALITY\", \"DRINKS#STYLE&OPTIONS\", \"RESTAURANT#PRICES\", \"RESTAURANT#GENERAL\", \"RESTAURANT#MISCELLANEOUS\", \"SERVICE#GENERAL\", \"AMBIENCE#GENERAL\", \"LOCATION#GENERAL\"], \"name\": \"Train\", \"textposition\": \"inside\", \"type\": \"pie\", \"values\": [1401, 2658, 1746, 102, 114, 102, 263, 855, 151, 795, 737, 373]}, {\"domain\": {\"x\": [0.35555555555555557, 0.6444444444444445], \"y\": [0.0, 1.0]}, \"labels\": [\"FOOD#PRICES\", \"FOOD#QUALITY\", \"FOOD#STYLE&OPTIONS\", \"DRINKS#PRICES\", \"DRINKS#QUALITY\", \"DRINKS#STYLE&OPTIONS\", \"RESTAURANT#PRICES\", \"RESTAURANT#GENERAL\", \"RESTAURANT#MISCELLANEOUS\", \"SERVICE#GENERAL\", \"AMBIENCE#GENERAL\", \"LOCATION#GENERAL\"], \"name\": \"Dev\", \"textposition\": \"inside\", \"type\": \"pie\", \"values\": [724, 1160, 516, 44, 39, 10, 35, 231, 117, 228, 205, 134]}, {\"domain\": {\"x\": [0.7111111111111111, 1.0], \"y\": [0.0, 1.0]}, \"labels\": [\"FOOD#PRICES\", \"FOOD#QUALITY\", \"FOOD#STYLE&OPTIONS\", \"DRINKS#PRICES\", \"DRINKS#QUALITY\", \"DRINKS#STYLE&OPTIONS\", \"RESTAURANT#PRICES\", \"RESTAURANT#GENERAL\", \"RESTAURANT#MISCELLANEOUS\", \"SERVICE#GENERAL\", \"AMBIENCE#GENERAL\", \"LOCATION#GENERAL\"], \"name\": \"Test\", \"textposition\": \"inside\", \"type\": \"pie\", \"values\": [331, 457, 403, 76, 71, 46, 73, 223, 130, 175, 255, 179]}],\n",
              "                        {\"annotations\": [{\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Train\", \"x\": 0.14444444444444446, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Dev\", \"x\": 0.5, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Test\", \"x\": 0.8555555555555556, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"# of samples per aspect\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dfc2e6a9-84dc-484d-a6ae-803f9191075b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQLRJkhS1t83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "26480b8e-5067-40da-a36f-abee18613c4d"
      },
      "source": [
        "phaseML = (ytrain_ml, ydev_ml, ytest_ml)\n",
        "\n",
        "fig = make_subplots(cols=3, subplot_titles=names,\n",
        "                    specs=[[{'type': 'pie'}] * 3])\n",
        "\n",
        "for i, (df, name) in enumerate(zip(phaseML, names), 1):\n",
        "    fig.add_trace(get_pie(df, name), row=1, col=i)\n",
        "\n",
        "fig.update_layout(title='# of samples per class (entity, sentiment)')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"14e0631a-d11c-4df4-89a1-96b7339d737a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"14e0631a-d11c-4df4-89a1-96b7339d737a\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '14e0631a-d11c-4df4-89a1-96b7339d737a',\n",
              "                        [{\"domain\": {\"x\": [0.0, 0.2888888888888889], \"y\": [0.0, 1.0]}, \"labels\": [\"FOOD#PRICES -\", \"FOOD#PRICES o\", \"FOOD#PRICES +\", \"FOOD#QUALITY -\", \"FOOD#QUALITY o\", \"FOOD#QUALITY +\", \"FOOD#STYLE&OPTIONS -\", \"FOOD#STYLE&OPTIONS o\", \"FOOD#STYLE&OPTIONS +\", \"DRINKS#PRICES -\", \"DRINKS#PRICES o\", \"DRINKS#PRICES +\", \"DRINKS#QUALITY -\", \"DRINKS#QUALITY o\", \"DRINKS#QUALITY +\", \"DRINKS#STYLE&OPTIONS -\", \"DRINKS#STYLE&OPTIONS o\", \"DRINKS#STYLE&OPTIONS +\", \"RESTAURANT#PRICES -\", \"RESTAURANT#PRICES o\", \"RESTAURANT#PRICES +\", \"RESTAURANT#GENERAL -\", \"RESTAURANT#GENERAL o\", \"RESTAURANT#GENERAL +\", \"RESTAURANT#MISCELLANEOUS -\", \"RESTAURANT#MISCELLANEOUS o\", \"RESTAURANT#MISCELLANEOUS +\", \"SERVICE#GENERAL -\", \"SERVICE#GENERAL o\", \"SERVICE#GENERAL +\", \"AMBIENCE#GENERAL -\", \"AMBIENCE#GENERAL o\", \"AMBIENCE#GENERAL +\", \"LOCATION#GENERAL -\", \"LOCATION#GENERAL o\", \"LOCATION#GENERAL +\"], \"name\": \"Train\", \"textposition\": \"inside\", \"type\": \"pie\", \"values\": [133, 566, 702, 53, 292, 2313, 88, 153, 1505, 13, 32, 57, 12, 10, 92, 4, 2, 96, 23, 74, 166, 25, 27, 803, 45, 23, 83, 110, 77, 608, 92, 132, 513, 72, 168, 133]}, {\"domain\": {\"x\": [0.35555555555555557, 0.6444444444444445], \"y\": [0.0, 1.0]}, \"labels\": [\"FOOD#PRICES -\", \"FOOD#PRICES o\", \"FOOD#PRICES +\", \"FOOD#QUALITY -\", \"FOOD#QUALITY o\", \"FOOD#QUALITY +\", \"FOOD#STYLE&OPTIONS -\", \"FOOD#STYLE&OPTIONS o\", \"FOOD#STYLE&OPTIONS +\", \"DRINKS#PRICES -\", \"DRINKS#PRICES o\", \"DRINKS#PRICES +\", \"DRINKS#QUALITY -\", \"DRINKS#QUALITY o\", \"DRINKS#QUALITY +\", \"DRINKS#STYLE&OPTIONS -\", \"DRINKS#STYLE&OPTIONS o\", \"DRINKS#STYLE&OPTIONS +\", \"RESTAURANT#PRICES -\", \"RESTAURANT#PRICES o\", \"RESTAURANT#PRICES +\", \"RESTAURANT#GENERAL -\", \"RESTAURANT#GENERAL o\", \"RESTAURANT#GENERAL +\", \"RESTAURANT#MISCELLANEOUS -\", \"RESTAURANT#MISCELLANEOUS o\", \"RESTAURANT#MISCELLANEOUS +\", \"SERVICE#GENERAL -\", \"SERVICE#GENERAL o\", \"SERVICE#GENERAL +\", \"AMBIENCE#GENERAL -\", \"AMBIENCE#GENERAL o\", \"AMBIENCE#GENERAL +\", \"LOCATION#GENERAL -\", \"LOCATION#GENERAL o\", \"LOCATION#GENERAL +\"], \"name\": \"Dev\", \"textposition\": \"inside\", \"type\": \"pie\", \"values\": [49, 302, 373, 28, 103, 1029, 50, 33, 433, 2, 13, 29, 3, 6, 30, 3, 0, 7, 2, 17, 16, 17, 9, 205, 8, 68, 41, 29, 15, 184, 33, 23, 149, 7, 103, 24]}, {\"domain\": {\"x\": [0.7111111111111111, 1.0], \"y\": [0.0, 1.0]}, \"labels\": [\"FOOD#PRICES -\", \"FOOD#PRICES o\", \"FOOD#PRICES +\", \"FOOD#QUALITY -\", \"FOOD#QUALITY o\", \"FOOD#QUALITY +\", \"FOOD#STYLE&OPTIONS -\", \"FOOD#STYLE&OPTIONS o\", \"FOOD#STYLE&OPTIONS +\", \"DRINKS#PRICES -\", \"DRINKS#PRICES o\", \"DRINKS#PRICES +\", \"DRINKS#QUALITY -\", \"DRINKS#QUALITY o\", \"DRINKS#QUALITY +\", \"DRINKS#STYLE&OPTIONS -\", \"DRINKS#STYLE&OPTIONS o\", \"DRINKS#STYLE&OPTIONS +\", \"RESTAURANT#PRICES -\", \"RESTAURANT#PRICES o\", \"RESTAURANT#PRICES +\", \"RESTAURANT#GENERAL -\", \"RESTAURANT#GENERAL o\", \"RESTAURANT#GENERAL +\", \"RESTAURANT#MISCELLANEOUS -\", \"RESTAURANT#MISCELLANEOUS o\", \"RESTAURANT#MISCELLANEOUS +\", \"SERVICE#GENERAL -\", \"SERVICE#GENERAL o\", \"SERVICE#GENERAL +\", \"AMBIENCE#GENERAL -\", \"AMBIENCE#GENERAL o\", \"AMBIENCE#GENERAL +\", \"LOCATION#GENERAL -\", \"LOCATION#GENERAL o\", \"LOCATION#GENERAL +\"], \"name\": \"Test\", \"textposition\": \"inside\", \"type\": \"pie\", \"values\": [28, 175, 128, 11, 43, 403, 16, 53, 334, 3, 45, 28, 6, 11, 54, 1, 4, 41, 5, 24, 44, 13, 5, 205, 9, 62, 59, 25, 22, 128, 26, 48, 181, 16, 99, 64]}],\n",
              "                        {\"annotations\": [{\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Train\", \"x\": 0.14444444444444446, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Dev\", \"x\": 0.5, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Test\", \"x\": 0.8555555555555556, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"# of samples per class (entity, sentiment)\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('14e0631a-d11c-4df4-89a1-96b7339d737a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ9lbnGt9Kd6"
      },
      "source": [
        "## Sample length distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHRo6N3cjlbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "91254518-f6c2-4711-ec38-31f0ea3a7caf"
      },
      "source": [
        "count0 = [len(s) for s in xtrain]\n",
        "count1 = [len(s) for s in xdev]\n",
        "count2 = [len(s) for s in xtest]\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=count0, name='train'))\n",
        "fig.add_trace(go.Histogram(x=count1, name='dev'))\n",
        "fig.add_trace(go.Histogram(x=count2, name='test'))\n",
        "\n",
        "fig.update_layout(title='Sample length distribution', barmode='overlay')\n",
        "fig.update_traces(opacity=0.5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"da4d5015-4775-42b4-9736-5d26484cbf11\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"da4d5015-4775-42b4-9736-5d26484cbf11\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'da4d5015-4775-42b4-9736-5d26484cbf11',\n",
              "                        [{\"name\": \"train\", \"opacity\": 0.5, \"type\": \"histogram\", \"x\": [323, 384, 268, 371, 314, 237, 292, 113, 345, 319, 22, 194, 573, 347, 180, 366, 412, 535, 581, 356, 94, 201, 73, 133, 187, 147, 167, 248, 226, 897, 397, 459, 803, 282, 180, 59, 197, 199, 264, 96, 35, 72, 127, 33, 55, 147, 532, 614, 390, 272, 825, 56, 20, 948, 357, 180, 335, 45, 223, 177, 64, 361, 108, 103, 110, 147, 91, 104, 125, 90, 42, 216, 87, 70, 61, 610, 485, 39, 808, 654, 631, 536, 502, 351, 324, 694, 222, 377, 903, 332, 620, 608, 445, 498, 1441, 447, 1633, 79, 22, 67, 810, 339, 282, 48, 102, 172, 10, 104, 400, 691, 553, 320, 2797, 400, 828, 634, 380, 389, 780, 1195, 715, 280, 452, 336, 334, 345, 178, 286, 458, 290, 316, 397, 746, 428, 235, 511, 283, 491, 637, 584, 424, 536, 340, 396, 41, 248, 301, 320, 107, 64, 39, 244, 256, 285, 238, 191, 42, 522, 219, 326, 357, 226, 258, 271, 140, 160, 252, 292, 298, 310, 161, 215, 249, 217, 83, 286, 327, 41, 148, 79, 34, 36, 78, 103, 54, 452, 141, 167, 127, 21, 533, 257, 34, 54, 242, 79, 79, 174, 86, 55, 139, 129, 254, 48, 183, 81, 192, 25, 20, 427, 61, 78, 56, 100, 200, 261, 324, 444, 130, 252, 330, 342, 418, 204, 257, 248, 264, 53, 285, 553, 103, 85, 101, 176, 253, 86, 52, 91, 95, 129, 412, 72, 151, 263, 343, 575, 26, 57, 182, 72, 191, 103, 57, 25, 113, 272, 53, 265, 117, 45, 41, 350, 250, 340, 92, 244, 18, 237, 256, 311, 298, 37, 321, 259, 61, 49, 166, 295, 74, 44, 268, 44, 212, 238, 85, 75, 123, 65, 120, 307, 228, 61, 50, 128, 88, 64, 36, 134, 178, 94, 152, 205, 172, 160, 301, 167, 108, 281, 135, 94, 210, 146, 326, 162, 203, 22, 59, 99, 187, 42, 48, 19, 246, 283, 89, 242, 438, 266, 213, 932, 45, 344, 305, 153, 51, 95, 350, 161, 29, 406, 319, 612, 447, 65, 679, 58, 462, 222, 289, 511, 162, 130, 281, 173, 343, 220, 197, 172, 328, 408, 76, 273, 22, 196, 320, 233, 421, 243, 37, 97, 172, 346, 390, 74, 299, 157, 210, 155, 302, 232, 341, 314, 238, 118, 248, 214, 243, 565, 231, 318, 220, 283, 99, 323, 354, 433, 667, 147, 222, 32, 133, 53, 174, 80, 46, 74, 259, 80, 215, 34, 273, 284, 492, 234, 55, 588, 60, 1008, 209, 279, 235, 132, 211, 32, 144, 103, 134, 194, 160, 122, 250, 160, 339, 150, 110, 98, 233, 381, 131, 185, 155, 120, 61, 182, 405, 248, 139, 292, 36, 154, 359, 176, 66, 213, 663, 88, 103, 165, 221, 85, 213, 106, 135, 212, 309, 63, 25, 40, 292, 130, 331, 184, 84, 71, 89, 285, 87, 34, 75, 16, 192, 36, 221, 9, 264, 24, 242, 49, 297, 238, 145, 267, 36, 427, 458, 287, 289, 103, 305, 159, 280, 42, 439, 310, 75, 97, 272, 233, 250, 309, 135, 25, 281, 40, 65, 510, 259, 202, 212, 328, 251, 187, 187, 66, 56, 232, 428, 784, 574, 393, 211, 78, 172, 68, 377, 330, 754, 311, 199, 1092, 294, 268, 493, 334, 1818, 235, 123, 370, 369, 240, 157, 33, 129, 185, 351, 214, 233, 235, 406, 169, 214, 81, 206, 118, 18, 297, 87, 544, 180, 78, 104, 200, 611, 94, 318, 550, 452, 158, 133, 95, 578, 123, 304, 67, 349, 269, 104, 457, 269, 79, 169, 216, 206, 231, 318, 239, 76, 102, 335, 272, 26, 104, 334, 108, 217, 463, 355, 147, 256, 265, 96, 277, 333, 39, 84, 329, 134, 234, 234, 317, 268, 127, 218, 240, 197, 164, 72, 232, 157, 246, 232, 304, 337, 382, 179, 304, 79, 174, 89, 93, 165, 18, 23, 176, 393, 136, 196, 69, 195, 149, 309, 164, 240, 308, 218, 123, 113, 282, 76, 72, 302, 151, 166, 102, 93, 306, 271, 529, 229, 261, 303, 131, 55, 144, 231, 264, 74, 153, 333, 67, 131, 258, 329, 168, 272, 151, 98, 322, 18, 61, 32, 211, 123, 45, 138, 141, 90, 286, 306, 160, 120, 468, 137, 254, 59, 118, 38, 141, 112, 803, 205, 323, 309, 241, 338, 78, 223, 468, 305, 279, 96, 271, 254, 389, 493, 130, 718, 265, 440, 363, 351, 160, 134, 107, 312, 83, 394, 322, 303, 211, 279, 608, 163, 299, 40, 33, 244, 32, 117, 761, 193, 253, 238, 456, 51, 241, 121, 89, 116, 482, 368, 191, 532, 157, 21, 308, 326, 137, 184, 141, 325, 219, 353, 356, 367, 13, 403, 254, 95, 118, 307, 92, 251, 200, 158, 185, 292, 24, 302, 397, 35, 266, 304, 87, 272, 246, 219, 278, 164, 73, 37, 72, 192, 94, 85, 151, 48, 162, 82, 72, 467, 362, 475, 268, 288, 487, 81, 58, 346, 218, 102, 488, 268, 935, 337, 200, 47, 58, 290, 235, 449, 271, 170, 243, 451, 289, 136, 465, 298, 327, 191, 103, 105, 35, 149, 74, 9, 11, 192, 27, 75, 108, 184, 147, 47, 14, 66, 530, 102, 130, 28, 24, 55, 288, 607, 178, 317, 177, 67, 125, 42, 99, 320, 402, 423, 513, 70, 39, 64, 324, 340, 220, 262, 174, 186, 70, 260, 451, 205, 485, 154, 264, 43, 300, 70, 376, 287, 344, 357, 285, 522, 704, 81, 153, 55, 39, 161, 559, 237, 310, 543, 84, 366, 434, 97, 202, 75, 75, 193, 375, 1165, 112, 152, 353, 303, 84, 72, 300, 66, 351, 468, 141, 349, 399, 102, 119, 183, 148, 283, 685, 1182, 117, 184, 445, 261, 235, 392, 361, 354, 168, 156, 396, 148, 409, 251, 469, 191, 163, 323, 401, 473, 370, 380, 292, 295, 268, 257, 323, 276, 62, 78, 176, 101, 66, 229, 10, 30, 69, 254, 136, 283, 195, 132, 283, 33, 25, 90, 42, 176, 367, 562, 424, 510, 231, 42, 152, 43, 131, 160, 167, 209, 286, 134, 158, 263, 409, 160, 547, 131, 41, 178, 63, 80, 99, 193, 117, 143, 270, 31, 91, 313, 154, 272, 282, 195, 237, 161, 213, 256, 129, 17, 275, 270, 56, 337, 106, 35, 271, 40, 135, 366, 991, 66, 26, 207, 228, 206, 65, 497, 273, 70, 34, 330, 92, 86, 298, 284, 309, 191, 299, 52, 478, 144, 317, 138, 444, 150, 46, 190, 478, 79, 297, 205, 140, 105, 207, 436, 95, 54, 473, 208, 333, 249, 337, 703, 63, 222, 462, 132, 50, 520, 253, 96, 302, 78, 769, 217, 174, 368, 27, 168, 129, 47, 267, 168, 144, 359, 56, 112, 442, 27, 209, 248, 20, 20, 268, 394, 328, 36, 425, 286, 47, 87, 82, 17, 235, 450, 684, 168, 272, 267, 346, 503, 184, 149, 986, 167, 343, 557, 332, 218, 295, 205, 321, 311, 375, 39, 378, 314, 305, 290, 71, 106, 221, 102, 274, 114, 446, 191, 54, 469, 288, 456, 295, 272, 235, 286, 199, 19, 6, 75, 12, 472, 520, 529, 312, 311, 259, 311, 212, 492, 218, 80, 117, 241, 221, 541, 249, 464, 84, 803, 302, 154, 328, 301, 191, 219, 370, 219, 115, 37, 413, 101, 274, 442, 98, 375, 213, 210, 137, 445, 231, 381, 1067, 551, 382, 50, 333, 57, 290, 54, 12, 27, 338, 309, 77, 52, 221, 92, 152, 225, 182, 187, 396, 144, 322, 51, 212, 8, 157, 106, 201, 27, 202, 55, 88, 12, 144, 36, 399, 207, 31, 124, 378, 272, 73, 232, 141, 53, 49, 707, 168, 147, 124, 59, 566, 42, 110, 21, 348, 380, 117, 274, 430, 280, 180, 152, 712, 324, 556, 95, 323, 119, 240, 443, 224, 122, 347, 277, 102, 25, 373, 408, 355, 246, 886, 302, 437, 265, 286, 33, 328, 484, 307, 153, 336, 254, 174, 50, 75, 186, 323, 203, 142, 132, 442, 124, 126, 71, 1792, 152, 613, 543, 447, 85, 308, 295, 403, 538, 275, 278, 220, 230, 242, 255, 304, 347, 359, 49, 737, 47, 637, 1433, 326, 398, 35, 210, 102, 365, 113, 622, 31, 190, 287, 199, 431, 50, 27, 308, 355, 369, 78, 83, 419, 347, 217, 292, 608, 531, 264, 72, 378, 216, 163, 595, 389, 121, 50, 252, 236, 84, 42, 78, 139, 238, 241, 311, 241, 286, 43, 224, 258, 425, 387, 700, 196, 277, 104, 125, 23, 26, 204, 74, 258, 105, 47, 175, 46, 365, 219, 30, 149, 282, 29, 246, 301, 48, 203, 232, 296, 339, 66, 69, 246, 122, 36, 231, 207, 79, 48, 43, 245, 311, 325, 248, 107, 348, 54, 16, 362, 50, 122, 331, 83, 273, 355, 370, 429, 288, 217, 329, 336, 72, 82, 104, 204, 117, 265, 109, 349, 184, 143, 524, 367, 341, 145, 468, 293, 433, 241, 272, 240, 476, 360, 341, 224, 325, 136, 86, 174, 82, 123, 218, 265, 47, 336, 89, 14, 247, 209, 51, 273, 120, 31, 332, 284, 80, 169, 46, 94, 201, 284, 308, 208, 518, 324, 258, 652, 41, 1015, 143, 145, 261, 335, 112, 1286, 271, 151, 883, 264, 361, 409, 571, 1572, 142, 443, 224, 190, 748, 385, 684, 277, 406, 57, 941, 307, 531, 887, 296, 250, 183, 271, 273, 419, 236, 263, 482, 400, 164, 213, 239, 111, 207, 54, 916, 177, 164, 172, 285, 327, 425, 356, 254, 291, 94, 166, 383, 198, 208, 193, 121, 344, 322, 300, 365, 243, 152, 231, 401, 503, 214, 222, 512, 51, 330, 376, 207, 178, 980, 207, 200, 16, 165, 41, 107, 226, 154, 90, 237, 312, 366, 506, 258, 152, 286, 494, 260, 26, 290, 323, 783, 144, 615, 77, 333, 354, 189, 273, 311, 93, 261, 86, 214, 57, 207, 31, 271, 376, 34, 43, 542, 36, 343, 99, 85, 156, 50, 166, 303, 223, 111, 43, 242, 312, 143, 371, 140, 254, 319, 357, 27, 73, 339, 84, 121, 536, 398, 391, 405, 241, 256, 73, 325, 296, 50, 134, 292, 210, 538, 181, 103, 229, 247, 264, 155, 151, 258, 202, 404, 277, 172, 295, 380, 291, 230, 359, 325, 237, 298, 88, 41, 91, 393, 220, 90, 242, 78, 189, 217, 107, 145, 32, 71, 207, 28, 334, 165, 299, 306, 73, 197, 277, 193, 303, 70, 52, 38, 65, 259, 120, 205, 355, 179, 299, 374, 290, 150, 173, 294, 287, 284, 263, 352, 365, 356, 199, 100, 91, 323, 353, 259, 299, 304, 175, 114, 271, 111, 303, 273, 274, 371, 317, 193, 220, 206, 35, 35, 196, 163, 64, 56, 28, 73, 12, 19, 274, 286, 216, 151, 160, 136, 88, 284, 365, 514, 663, 217, 205, 130, 289, 92, 169, 94, 519, 720, 901, 64, 138, 179, 40, 376, 17, 8, 233, 324, 51, 202, 233, 327, 333, 351, 176, 435, 147, 400, 298, 256, 458, 60, 123, 901, 304, 505, 301, 1525, 336, 387, 399, 1441, 222, 397, 85, 240, 318, 283, 155, 222, 196, 719, 429, 252, 408, 330, 17, 79, 539, 276, 303, 95, 209, 203, 183, 313, 354, 4, 4, 17, 19, 144, 54, 60, 33, 24, 23, 201, 75, 10, 13, 99, 237, 55, 10, 10, 113, 88, 81, 17, 10, 8, 123, 107, 107, 162, 36, 8, 15, 52, 128, 107, 175, 16, 8, 23, 14, 30, 20, 139, 182, 55, 256, 16, 37, 220, 61, 21, 17, 262, 8, 45, 114, 9, 59, 32, 35, 41, 19, 49, 17, 15, 87, 80, 309, 32, 145, 108, 14, 40, 105, 86, 23, 91, 15, 30, 246, 95, 28, 21, 19, 19, 60, 41, 10, 31, 340, 238, 14, 14, 48, 19, 19, 26, 7, 20, 63, 26, 264, 48, 69, 17, 21, 338, 30, 299, 67, 56, 407, 423, 228, 328, 17, 89, 309, 27, 22, 32, 58, 57, 29, 74, 186, 24, 33, 68, 60, 14, 24, 38, 21, 112, 10, 10, 204, 72, 21, 33, 76, 55, 10, 73, 245, 82, 106, 18, 147, 99, 10, 22, 18, 438, 49, 183, 91, 115, 355, 180, 54, 18, 8, 20, 24, 278, 28, 88, 29, 26, 46, 78, 104, 291, 4, 121, 40, 46, 31, 237, 60, 115, 150, 181, 13, 24, 8, 204, 47, 29, 202, 257, 199, 136, 431, 733, 405, 243, 84, 183, 197, 328, 465, 268, 256, 947, 283, 113, 490, 22, 96, 1091, 103, 162, 313, 170, 120, 323, 225, 463, 599, 22, 290, 113, 960, 54, 303, 949, 58, 310, 22, 153, 243, 220, 45, 189, 338, 89, 24, 280, 213, 511, 57, 456, 159, 18, 84, 82, 65, 693, 150, 439, 233, 333, 374, 347, 82, 131, 82, 103, 29, 372, 252, 386, 310, 157, 497, 258, 79, 385, 272, 285, 41, 408, 36, 109, 229, 97, 306, 100, 216, 26, 53, 442, 207, 402, 31, 256, 259, 98, 43, 350, 174, 279, 232, 341, 55, 71, 283, 161, 157, 274, 208, 359, 223, 103, 81, 206, 72, 58, 201, 247, 148, 121, 115, 170, 95, 529, 115, 118, 402, 302, 449, 393, 370, 331, 294, 246, 405, 252, 338, 260, 265, 181, 266, 266, 454, 53, 588, 308, 243, 123, 151, 325, 393, 393, 248, 225, 195, 247, 267, 267, 197, 40, 282, 213, 194, 241, 253, 594, 195, 63, 30, 181, 74, 599, 68, 239, 291, 356, 335, 261, 29, 141, 139, 149, 237, 186, 401, 245, 533, 81, 305, 344, 342, 81, 647, 310, 197, 712, 189, 404, 281, 36, 298, 31, 355, 235, 63, 30, 340, 148, 24, 554, 324, 535, 687, 362, 174, 219, 247, 484, 102, 1598, 285, 167, 582, 448, 280, 471, 290, 233, 359, 278, 155, 90, 168, 272, 170, 443, 155, 638, 272, 28, 405, 115, 123, 136, 78, 18, 181, 435, 63, 261, 95, 78, 304, 30, 282, 300, 218, 246, 216, 242, 266, 672, 32, 80, 365, 25, 140, 326, 232, 212, 96, 217, 59, 484, 72, 212, 231, 20, 294, 73, 52, 64, 189, 26, 318, 305, 35, 34, 581, 162, 329, 112, 534, 96, 408, 263, 255, 90, 344, 347, 47, 315, 461, 52, 179, 361, 418, 444, 440, 262, 152, 158, 198, 34, 299, 434, 24, 196, 172, 164, 430, 1138, 271, 346, 162, 118, 577, 449, 41, 223, 668, 194, 302, 99, 466, 1104, 155, 289, 397, 585, 243, 263, 398, 208, 397, 240, 388, 42, 269, 594, 202, 366, 306, 271, 349, 316, 260, 35, 345, 302, 404, 125, 555, 330, 197, 346, 557, 130, 94, 61, 357, 133, 704, 437, 405, 458, 384, 253, 181, 187, 308, 1017, 669, 176, 70, 203, 249, 396, 267, 273, 367, 260, 347, 101, 333, 355, 239, 799, 421, 209, 352, 762, 101, 344, 259, 39, 214, 88, 149, 43, 786, 305, 275, 241, 500, 233, 459, 209, 318, 194, 187, 215, 136, 260, 205, 231, 556, 389, 321, 176, 130, 269, 234, 349, 189, 538, 391, 208, 298, 200, 100, 205, 173, 442, 276, 380, 299, 122, 255, 280, 460, 434, 231, 280, 204, 16, 101, 188, 53, 202, 292, 206, 393, 243, 234, 219, 235, 109, 164, 103, 975, 156, 278, 238, 305, 302, 266, 80, 88, 343, 166, 367, 376, 228, 257, 415, 205, 251, 207, 282, 142, 229, 712, 176, 258, 423, 248, 93, 72, 45, 744, 119, 398, 62, 149, 351, 407, 660, 403, 398, 172, 233, 84, 326, 285, 185, 258, 78, 216, 440, 279, 597, 146, 415, 314, 255, 65, 36, 338, 56, 9, 284, 22, 114, 96, 320, 44, 56, 209, 33, 135, 166, 17, 96, 666, 228, 482, 22, 158, 33, 53, 391, 109, 165, 621, 201, 606, 375, 212, 426, 521, 251, 194, 217, 375, 129, 85, 121, 180, 202, 293, 19, 17, 115, 115, 123, 7, 139, 214, 253, 175, 99, 16, 76, 436, 300, 301, 152, 114, 399, 131, 289, 316, 25, 33, 100, 176, 209, 227, 35, 157, 388, 520, 156, 158, 18, 303, 396, 184, 436, 244, 344, 142, 74, 59, 89, 569, 228, 110, 135, 415, 116, 540, 332, 38, 32, 330, 33, 203, 31, 282, 234, 225, 243, 275, 32, 193, 151, 81, 378, 344, 252, 110, 63, 101, 516, 41, 332, 368, 525, 372, 184, 825, 51, 262, 73, 716, 186, 466, 200, 315, 334, 613, 56, 352, 269, 47, 250, 83, 177, 131, 251, 243, 64, 356, 76, 2460, 360, 219, 150, 195, 267, 300, 186, 413, 243, 178, 249, 105, 341, 197, 65, 79, 29, 57, 62, 18, 222, 241, 49, 105, 32, 279, 139, 118, 31, 313, 290, 177, 73, 100, 36, 284, 175, 432, 303, 576, 206, 210, 5, 80, 384, 244, 412, 173, 341, 265, 160, 164, 206, 213, 83, 347, 91, 133, 170, 643, 342, 266, 208, 357, 210, 666, 84, 575, 140, 206, 312, 99, 110, 65, 311, 429, 173, 506, 144, 259, 395, 607, 392, 174, 348, 235, 354, 236, 294, 351, 561, 267, 224, 464, 358, 429, 456, 118, 13, 440, 93, 142, 120, 161, 149, 262, 169, 114, 70, 176, 199, 193, 449, 418, 69, 31, 297, 414, 194, 200, 190, 103, 240, 81, 93, 396, 219, 204, 94, 14, 75, 363, 306, 177, 653, 238, 132, 693, 243, 20, 218, 74, 592, 310, 474, 191, 916, 189, 204, 1925, 215, 554, 109, 300, 36, 74, 9, 91, 22, 179, 35, 158, 311, 64, 47, 428, 430, 16, 16, 169, 197, 377, 330, 155, 363, 268, 410, 477, 134, 54, 242, 112, 82, 41, 110, 216, 515, 117, 252, 176, 152, 62, 56, 126, 397, 299, 365, 141, 322, 653, 311, 170, 608, 313, 54, 720, 115, 284, 80, 89, 16, 80, 24, 36, 176, 823, 118, 104, 375, 276, 143, 230, 16, 282, 475, 525, 78, 192, 196, 175, 103, 235, 105, 554, 95, 340, 259, 77, 293, 45, 20, 324, 34, 283, 254]}, {\"name\": \"dev\", \"opacity\": 0.5, \"type\": \"histogram\", \"x\": [278, 328, 563, 72, 106, 472, 402, 189, 21, 515, 120, 142, 407, 122, 53, 905, 872, 259, 6, 92, 271, 57, 69, 477, 623, 862, 1294, 287, 898, 956, 195, 696, 194, 1072, 703, 566, 224, 713, 47, 653, 24, 208, 91, 397, 192, 107, 309, 71, 85, 214, 165, 26, 53, 228, 131, 95, 173, 198, 89, 406, 454, 602, 54, 82, 303, 38, 683, 347, 198, 160, 451, 346, 422, 32, 214, 388, 365, 298, 332, 114, 114, 54, 18, 83, 64, 127, 65, 32, 128, 93, 260, 325, 258, 195, 288, 204, 486, 73, 421, 141, 414, 68, 167, 292, 273, 313, 197, 113, 130, 148, 302, 48, 134, 84, 213, 1602, 301, 288, 59, 541, 3852, 279, 962, 691, 390, 279, 181, 231, 264, 421, 249, 112, 517, 417, 257, 371, 479, 319, 333, 342, 192, 284, 750, 353, 93, 756, 1393, 461, 710, 240, 365, 757, 421, 849, 580, 495, 438, 472, 586, 391, 425, 351, 347, 412, 591, 348, 767, 371, 100, 314, 319, 616, 456, 58, 422, 224, 584, 92, 135, 236, 318, 400, 317, 240, 437, 333, 9, 106, 650, 70, 23, 582, 11, 136, 141, 166, 283, 193, 145, 118, 172, 31, 287, 268, 154, 225, 43, 285, 213, 249, 132, 184, 203, 113, 367, 128, 215, 428, 288, 89, 256, 357, 138, 270, 260, 208, 233, 125, 241, 271, 257, 231, 247, 379, 136, 305, 236, 378, 47, 243, 142, 221, 284, 278, 246, 309, 233, 184, 229, 411, 366, 248, 430, 189, 148, 87, 109, 53, 52, 151, 186, 108, 303, 69, 185, 101, 27, 78, 113, 28, 298, 432, 292, 39, 112, 37, 74, 87, 378, 43, 150, 128, 210, 300, 450, 264, 140, 232, 331, 227, 155, 304, 41, 59, 20, 70, 88, 98, 358, 194, 153, 184, 92, 141, 174, 139, 187, 68, 103, 104, 129, 514, 333, 242, 38, 154, 324, 74, 80, 22, 57, 279, 198, 81, 20, 86, 245, 217, 541, 229, 39, 418, 442, 171, 268, 42, 65, 35, 126, 32, 62, 239, 45, 355, 20, 129, 179, 22, 391, 273, 71, 244, 103, 348, 78, 157, 217, 183, 289, 367, 45, 19, 103, 38, 288, 350, 207, 108, 101, 252, 213, 110, 238, 35, 194, 161, 396, 109, 195, 207, 96, 366, 307, 699, 275, 127, 83, 154, 472, 388, 32, 246, 309, 132, 215, 77, 70, 42, 158, 282, 254, 14, 86, 94, 212, 70, 52, 230, 24, 73, 26, 514, 229, 253, 263, 30, 129, 184, 78, 295, 25, 296, 80, 479, 147, 228, 450, 281, 152, 68, 77, 32, 144, 65, 33, 34, 287, 197, 81, 72, 244, 210, 20, 195, 22, 153, 180, 12, 57, 69, 220, 257, 185, 10, 84, 11, 17, 44, 51, 23, 176, 19, 46, 34, 59, 293, 59, 66, 53, 343, 152, 283, 251, 307, 193, 41, 92, 111, 62, 115, 185, 156, 136, 189, 121, 197, 83, 79, 27, 122, 23, 33, 466, 54, 15, 144, 13, 71, 42, 190, 329, 144, 53, 311, 139, 1927, 165, 356, 293, 212, 80, 67, 329, 139, 67, 45, 120, 40, 88, 139, 180, 266, 65, 139, 44, 277, 71, 263, 45, 18, 122, 13, 11, 32, 126, 289, 193, 30, 239, 165, 22, 48, 341, 70, 54, 216, 46, 265, 257, 54, 20, 566, 64, 26, 109, 114, 47, 181, 240, 135, 202, 158, 145, 154, 120, 214, 176, 402, 376, 230, 152, 72, 438, 99, 45, 135, 1138, 145, 19, 76, 169, 52, 173, 15, 247, 171, 197, 50, 52, 48, 176, 406, 80, 102, 81, 154, 366, 155, 223, 171, 153, 28, 22, 138, 172, 61, 521, 313, 121, 157, 45, 144, 261, 64, 124, 108, 315, 214, 237, 323, 73, 338, 107, 47, 272, 31, 49, 291, 350, 224, 263, 184, 305, 219, 289, 164, 278, 270, 230, 278, 438, 303, 174, 311, 290, 90, 491, 350, 55, 145, 387, 275, 157, 32, 42, 288, 67, 79, 50, 55, 47, 84, 76, 68, 86, 48, 205, 28, 462, 199, 93, 178, 167, 112, 243, 36, 64, 226, 85, 620, 303, 255, 246, 82, 256, 17, 114, 51, 310, 42, 241, 301, 187, 79, 46, 367, 114, 114, 271, 244, 483, 110, 1512, 147, 53, 110, 146, 227, 242, 804, 84, 142, 206, 182, 204, 173, 58, 400, 440, 52, 239, 79, 236, 140, 217, 52, 424, 377, 77, 298, 61, 74, 247, 160, 292, 238, 105, 193, 352, 405, 309, 256, 203, 94, 28, 178, 92, 209, 143, 111, 173, 63, 108, 411, 396, 166, 331, 252, 86, 241, 244, 311, 227, 216, 255, 127, 223, 92, 151, 81, 39, 70, 84, 259, 285, 137, 195, 144, 250, 310, 243, 160, 284, 117, 99, 111, 294, 219, 66, 72, 452, 81, 143, 76, 156, 102, 239, 195, 139, 76, 45, 238, 230, 35, 110, 287, 94, 123, 139, 167, 307, 103, 177, 388, 165, 158, 17, 257, 57, 281, 236, 118, 522, 195, 56, 165, 298, 180, 223, 204, 99, 457, 569, 140, 166, 188, 190, 211, 29, 200, 33, 103, 32, 188, 204, 328, 135, 117, 275, 151, 59, 97, 546, 123, 212, 312, 305, 98, 178, 78, 309, 139, 137, 182, 193, 125, 332, 166, 337, 347, 71, 103, 620, 201, 16, 122, 122, 305, 268, 253, 44, 28, 352, 40, 305, 66, 316, 197, 181, 99, 197, 116, 321, 288, 117, 326, 156, 98, 97, 135, 169, 75, 119, 230, 191, 118, 431, 225, 178, 19, 109, 262, 249, 115, 38, 51, 100, 151, 266, 58, 217, 261, 254, 55, 45, 141, 128, 153, 274, 305, 290, 585, 2, 7, 41, 756, 256, 216, 199, 470, 385, 299, 259, 418, 131, 269, 134, 305, 297, 838, 240, 78, 192, 403, 33, 226, 185, 325, 314, 181, 169, 52, 339, 128, 43, 46, 130, 214, 271, 81, 606, 341, 65, 48, 794, 246, 76, 157, 244, 116, 83, 235, 429, 80, 57, 155, 79, 227, 295, 95, 233, 107, 188, 122, 488, 262, 28, 40, 208, 75, 75, 159, 87, 159, 332, 527, 285, 158, 103, 38, 43, 315, 310, 77, 48, 241, 123, 111, 417, 320, 66, 407, 254, 138, 76, 72, 180, 29, 157, 68, 384, 109, 65, 245, 410, 266, 574, 142, 132, 244, 452, 74, 190, 154, 319, 294, 137, 80, 132, 203, 298, 274, 129, 212, 333, 96, 133, 233, 160, 184, 405, 173, 37, 268, 42, 23, 221, 318, 318, 192, 258, 126, 55, 134, 577, 317, 102, 283, 85, 51, 155, 36, 309, 144, 214, 186, 107, 357, 277, 308, 293, 232, 383, 37, 224, 54, 297, 72, 223, 229, 212, 250, 201, 55, 74, 56, 98, 292, 301, 108, 217, 219, 329, 288, 349, 102, 70, 356, 99, 127, 53, 368, 250, 62, 191, 84, 73, 196, 386, 181, 54, 183, 221, 155, 403, 113, 158, 313, 277, 25, 193, 63, 282, 286, 374, 304, 260, 149, 256, 94, 232, 118, 179, 114, 395, 322, 155, 76, 195, 200, 85, 141, 294, 426, 21, 62, 213, 348, 172, 697, 429, 174, 448, 114, 313, 277, 197, 71, 173, 244, 458, 345, 115, 359, 426, 64, 206, 391, 285, 71, 266, 97, 368, 268, 587, 200, 78, 686, 41, 44, 81, 1214, 334, 53, 88, 596, 199, 346, 296, 251, 230, 559, 156, 46, 181, 389, 109, 253, 491, 23, 148, 143, 68, 288, 114, 202, 101, 401, 356, 299, 311, 266, 239, 458, 384, 136, 114, 250, 254, 192, 350, 434, 98, 54, 297, 208, 63, 181, 54, 166, 869, 274, 181, 177, 135, 304, 426, 296, 279, 61, 273, 144, 1228, 372, 59, 119, 751, 470, 281, 332, 240, 67, 379, 238, 117, 590, 283, 200, 109, 45, 173, 81, 41, 185, 132, 120, 17, 57, 259, 64]}, {\"name\": \"test\", \"opacity\": 0.5, \"type\": \"histogram\", \"x\": [1122, 1188, 1277, 825, 169, 215, 217, 189, 1086, 896, 1360, 1310, 1247, 512, 831, 1380, 322, 404, 1237, 952, 771, 286, 1405, 1239, 288, 738, 377, 1300, 1323, 733, 1004, 646, 323, 708, 694, 888, 1173, 511, 790, 892, 842, 172, 753, 990, 1093, 890, 416, 785, 1223, 603, 832, 926, 588, 282, 721, 484, 199, 150, 1148, 1408, 954, 552, 1136, 1078, 165, 1292, 373, 528, 1086, 367, 1048, 336, 202, 957, 1178, 197, 127, 146, 559, 1310, 685, 1104, 1148, 968, 1264, 429, 360, 358, 373, 403, 254, 779, 245, 429, 804, 336, 985, 662, 681, 1255, 1395, 944, 1446, 638, 687, 271, 1251, 736, 1083, 989, 603, 986, 1375, 1012, 1132, 583, 1037, 261, 1244, 1061, 766, 705, 202, 1226, 438, 719, 135, 208, 465, 738, 603, 1379, 201, 1272, 944, 754, 783, 527, 444, 741, 751, 322, 394, 754, 257, 468, 1390, 908, 446, 786, 665, 564, 147, 1187, 1376, 343, 959, 778, 871, 1062, 238, 56, 778, 1414, 465, 1256, 691, 139, 1128, 807, 762, 816, 394, 1059, 953, 889, 1366, 524, 1190, 633, 563, 1152, 1139, 521, 516, 729, 785, 857, 1297, 743, 898, 284, 326, 1174, 808, 1619, 421, 1358, 1033, 283, 1362, 1210, 659, 569, 711, 170, 330, 1318, 210, 591, 1123, 221, 1412, 893, 217, 53, 1080, 655, 303, 330, 1315, 1298, 924, 757, 594, 575, 1151, 747, 850, 240, 168, 1264, 1115, 675, 277, 743, 1282, 868, 1360, 363, 392, 1376, 303, 127, 946, 944, 1210, 1010, 479, 783, 1222, 1003, 759, 295, 351, 680, 877, 1010, 247, 96, 280, 320, 181, 357, 628, 197, 939, 466, 1445, 1308, 1196, 720, 524, 1143, 601, 444, 210, 440, 1365, 884, 319, 637, 588, 1089, 811, 979, 1412, 456, 752, 1202, 958, 309, 1267, 926, 82, 1007, 886, 724, 896, 842, 744, 870, 856, 961, 1041, 591, 680, 141, 273, 360, 300, 682, 362, 207, 365, 363, 563, 1000, 341, 702, 723, 1165, 707, 844, 672, 856, 1393, 221, 1002, 443, 180, 244, 881, 596, 1013, 146, 334, 782, 915, 726, 680, 487, 342, 1432, 907, 316, 988, 178, 394, 1275, 634, 851, 48, 549, 1246, 1009, 1214, 852, 1036, 1158, 1248, 1084, 564, 822, 412, 696, 1239, 1113, 741, 1395, 676, 1409, 682, 113, 789, 299, 345, 1173, 1163, 754, 944, 985, 705, 939, 1129, 1167, 783, 797, 1042, 141, 205, 1003, 1263, 715, 374, 1280, 855, 1150, 1047, 737, 426, 730, 539, 348, 1134, 434, 208, 1436, 215, 1374, 334, 239, 134, 1249, 1160, 1463, 1173, 376, 445, 678, 551, 1779, 515, 840, 480, 1084, 360, 828, 738, 836, 1302, 308, 648, 1277, 1301, 1374, 298, 380, 126, 95, 219, 460, 669, 1347, 798, 873, 1295, 1193, 853, 352, 614, 341, 630, 446, 144, 1102, 1034, 781, 824, 1363, 969, 875, 1203, 1102, 636, 1096, 877, 1081, 542, 418, 812, 871, 577, 107, 781, 720, 1232, 441, 437, 734, 827, 541, 603, 1052, 412, 152, 762, 263, 979, 728, 1443, 669, 647, 960, 1154, 292, 1040, 770, 254, 217]}],\n",
              "                        {\"barmode\": \"overlay\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Sample length distribution\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('da4d5015-4775-42b4-9736-5d26484cbf11');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction (Convert reviews to vectors)"
      ],
      "metadata": {
        "id": "tz4aAURfgKIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Basic features (1, 2, 3 grams)\n"
      ],
      "metadata": {
        "id": "v2UquVpDgbvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
        "                             min_df=2, max_df=0.9)\n",
        "\n",
        "# x data using basic clean up class and basic features extrator\n",
        "xtrain_basecl_basef = vectorizer.fit_transform(xtrain_basecl)\n",
        "xdev_basecl_basef   = vectorizer.transform(xdev_basecl)\n",
        "xtest_basecl_basef  = vectorizer.transform(xtest_basecl)\n",
        "\n",
        "# x data using advanced clean up class and basic features extrator\n",
        "xtrain_basef = vectorizer.fit_transform(xtrain)\n",
        "xdev_basef   = vectorizer.transform(xdev)\n",
        "xtest_basef  = vectorizer.transform(xtest)"
      ],
      "metadata": {
        "id": "YTpdlwwCgY2o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_basecl_basef.shape, xtrain_basef.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egBRGMSHncKX",
        "outputId": "afd71d19-8ef2-4b1a-cc00-2600468ed754"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2961, 33802), (2961, 32971))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More features (pos tag, result from phase a)"
      ],
      "metadata": {
        "id": "DLDi1017gv5u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTCMGPWSpUD7"
      },
      "source": [
        "from underthesea import pos_tag, word_tokenize\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import (CountVectorizer,\n",
        "                                             TfidfTransformer,\n",
        "                                             TfidfVectorizer)\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.for_phase_a = True\n",
        "\n",
        "        # POS tags encoding\n",
        "        self.allow_tags = {tag: i for i, tag in enumerate('NVA', 1)}\n",
        "\n",
        "    def _nva_extractor(self, X):\n",
        "        \"\"\"Extract noun, verb, adjective tokens and tags\n",
        "        \"\"\"\n",
        "        reviews = []\n",
        "        for x in X:\n",
        "            review = [item for item in pos_tag(x) \\\n",
        "                      if item[1] in self.allow_tags]\n",
        "            reviews.append(review)\n",
        "        return reviews\n",
        "\n",
        "    def _postag_vtrz(self, reviews):\n",
        "        \"\"\"Convert pos tags to a feature matrix\n",
        "        \"\"\"\n",
        "        vocab = self.tfidf_nva[0].vocabulary\n",
        "        features = np.zeros((len(reviews), len(vocab)))\n",
        "        for review, feature in zip(reviews, features):\n",
        "            for token, tag in review:\n",
        "                try:\n",
        "                    feature[vocab[token]] = self.allow_tags[tag]\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        return features\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        reviews = self._nva_extractor(X)\n",
        "\n",
        "        # Create the vocabulary\n",
        "        vocab = {item[0] for review in reviews\n",
        "                 for item in review}\n",
        "        vocab = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "        # Tfidf for noun, verb, adjective tokens\n",
        "        count_nva = CountVectorizer(tokenizer=word_tokenize,\n",
        "                                    vocabulary=vocab,\n",
        "                                    min_df=2, max_df=0.9)\n",
        "        tfidf_vec = TfidfTransformer()\n",
        "        self.tfidf_nva = make_pipeline(count_nva, tfidf_vec).fit(X)\n",
        "\n",
        "        # 1, 2, 3 grams\n",
        "        self.tfidf_123 = TfidfVectorizer(ngram_range=(1, 3),\n",
        "                                         min_df=2, max_df=0.9).fit(X)\n",
        "        \n",
        "        # 2, 3 grams\n",
        "        self.tfidf_23 = TfidfVectorizer(ngram_range=(2, 3),\n",
        "                                        min_df=2, max_df=0.9).fit(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        reviews = self._nva_extractor(X)\n",
        "\n",
        "        if self.for_phase_a:\n",
        "            features = [self.tfidf_123.transform(X),\n",
        "                        self.tfidf_nva.transform(X),\n",
        "                        self._postag_vtrz(reviews)] \n",
        "            return sparse.hstack(features)\n",
        "\n",
        "        features = [self.tfidf_23.transform(X),\n",
        "                    self.tfidf_nva.transform(X),\n",
        "                    [['!' in text or '?' in text] for text in X]]\n",
        "        return sparse.hstack(features)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhpN1Qg_-hqI"
      },
      "source": [
        "# x data using advanced clean up class and advanced feature extractor\n",
        "fe = FeatureExtractor().fit(xtrain)\n",
        "\n",
        "fe.for_phase_a = True\n",
        "xtrain_a = fe.transform(xtrain)\n",
        "xdev_a   = fe.transform(xdev)\n",
        "xtest_a  = fe.transform(xtest)\n",
        "\n",
        "fe.for_phase_a = False\n",
        "xtrain_b = fe.transform(xtrain)\n",
        "xdev_b   = fe.transform(xdev)\n",
        "xtest_b  = fe.transform(xtest)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_a.shape, xtrain_b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_ud8b5kqt4x",
        "outputId": "1cc181fa-93e9-4349-ce0f-0b7686a583ae"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2961, 49621), (2961, 38205))"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00008-3e35d1d6-b9cd-4dc2-a45e-ccf833534cd9",
        "deepnote_cell_type": "markdown",
        "id": "ppKYl2LZPra0",
        "tags": []
      },
      "source": [
        "# Model Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-End Architecture"
      ],
      "metadata": {
        "id": "rFrqY_VnqN-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multioutput import MultiOutputClassifier as MOC"
      ],
      "metadata": {
        "id": "Hpwu1HJLqWmT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Idp3m802k5"
      },
      "source": [
        "## TwoStageSimple Architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "\n",
        "def add_features(X, y):\n",
        "    return scipy.sparse.hstack((X, y))\n",
        "\n",
        "class TwoStageSimple:\n",
        "    def __init__(self, model_a, model_b):\n",
        "        self.model_a = model_a \n",
        "        self.model_b = model_b\n",
        "\n",
        "    def fit(self, X, y_a, y_b):\n",
        "        self.model_a.fit(X, y_a)\n",
        "        X = add_features(X, y_a)\n",
        "        self.model_b.fit(X, y_b)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        y_a = self.model_a.predict(X) \n",
        "        X = add_features(X, y_a)\n",
        "        return self.model_b.predict(X)"
      ],
      "metadata": {
        "id": "5hUozgnutOl9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TwoStageAdvanced Architecture"
      ],
      "metadata": {
        "id": "Dy8xb9fmtVo9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kXQwG5RUL7F"
      },
      "source": [
        "class TwoStageAdvanced(TwoStageSimple):\n",
        "    def fit(self, X_a, X_b, y_a, y_b):\n",
        "        self.model_a.fit(X_a, y_a)\n",
        "        X = add_features(X_b, y_a)\n",
        "        self.model_b.fit(X, y_b)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X_a, X_b):\n",
        "        y_a = self.model_a.predict(X_a)\n",
        "        X = add_features(X_b, y_a)\n",
        "        pred = self.model_b.predict(X)\n",
        "        return pred"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00011-0d327c11-f739-49f4-b198-560e8b9d6e52",
        "deepnote_cell_type": "markdown",
        "id": "J7cEYevzPrbB",
        "tags": []
      },
      "source": [
        "# Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ952C7RUM6Y"
      },
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "\n",
        "def quick_f1(y_true, y_pred):\n",
        "    y_pred = mo2ml(mo2df(y_pred))\n",
        "    return round(f1_score(y_true, y_pred, average='micro', zero_division=0), 4)\n",
        "\n",
        "def evaluate(model, X, y, average='micro'):\n",
        "    yb_true  = mo2ml(y)\n",
        "\n",
        "    yb_pred  = mo2df(model.predict(X))\n",
        "    yb_pred  = mo2ml(yb_pred)\n",
        "\n",
        "    return classification_report(yb_true, yb_pred, zero_division=0)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare models"
      ],
      "metadata": {
        "id": "1mOvkWcI-j4u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6quUiO0590xf"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS1uwzHL2FEq"
      },
      "source": [
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf0 = MOC(LinearSVC(random_state=5))\n",
        "clf0.fit(xtrain_basecl_basef, ytrain_b)\n",
        "\n",
        "print(quick_f1(ytrain_ml, clf0.predict(xtrain_basecl_basef)))\n",
        "print(quick_f1(ydev_ml  , clf0.predict(xdev_basecl_basef)))\n",
        "print(quick_f1(ytest_ml , clf0.predict(xtest_basecl_basef)))\n",
        "print(evaluate(clf0, xtest_basecl_basef, ytest_b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL6MjQ6kmLwm",
        "outputId": "d5ac3cc1-3e37-4ce8-aaea-f96adfe21a2a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.658\n",
            "0.6063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.04      0.06        28\n",
            "           1       0.51      0.37      0.43       175\n",
            "           2       0.47      0.66      0.55       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.69      0.21      0.32        43\n",
            "           5       0.85      0.99      0.91       403\n",
            "           6       0.00      0.00      0.00        16\n",
            "           7       0.00      0.00      0.00        53\n",
            "           8       0.74      0.96      0.83       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.00      0.00      0.00        45\n",
            "          11       0.00      0.00      0.00        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.91      0.19      0.31        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.71      0.12      0.21        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.00      0.00      0.00        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.58      0.73      0.64       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       1.00      0.02      0.03        59\n",
            "          27       0.50      0.04      0.07        25\n",
            "          28       1.00      0.05      0.09        22\n",
            "          29       0.64      0.63      0.64       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.00      0.00      0.00        48\n",
            "          32       0.86      0.76      0.80       181\n",
            "          33       1.00      0.12      0.22        16\n",
            "          34       0.87      0.20      0.33        99\n",
            "          35       0.57      0.06      0.11        64\n",
            "\n",
            "   micro avg       0.70      0.53      0.61      2419\n",
            "   macro avg       0.34      0.17      0.18      2419\n",
            "weighted avg       0.59      0.53      0.51      2419\n",
            " samples avg       0.70      0.55      0.60      2419\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf1 = MOC(LinearSVC(random_state=5))\n",
        "clf1.fit(xtrain_basef, ytrain_b)\n",
        "\n",
        "print(quick_f1(ytrain_ml, clf1.predict(xtrain_basef)))\n",
        "print(quick_f1(ydev_ml  , clf1.predict(xdev_basef)))\n",
        "print(quick_f1(ytest_ml , clf1.predict(xtest_basef)))\n",
        "print(evaluate(clf1, xtest_basef, ytest_b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1SstOPPBaC4",
        "outputId": "9c997943-2ff8-468c-f7e7-e305f7ce8f51"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9989\n",
            "0.6631\n",
            "0.6105\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.04      0.06        28\n",
            "           1       0.58      0.43      0.49       175\n",
            "           2       0.48      0.66      0.56       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.69      0.21      0.32        43\n",
            "           5       0.84      0.99      0.91       403\n",
            "           6       0.00      0.00      0.00        16\n",
            "           7       0.00      0.00      0.00        53\n",
            "           8       0.74      0.96      0.84       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.00      0.00      0.00        45\n",
            "          11       0.00      0.00      0.00        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.92      0.20      0.33        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.75      0.15      0.24        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.33      0.02      0.04        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.56      0.72      0.63       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       0.50      0.02      0.03        59\n",
            "          27       1.00      0.04      0.08        25\n",
            "          28       1.00      0.05      0.09        22\n",
            "          29       0.62      0.65      0.63       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.00      0.00      0.00        48\n",
            "          32       0.84      0.76      0.80       181\n",
            "          33       0.75      0.19      0.30        16\n",
            "          34       0.90      0.18      0.30        99\n",
            "          35       0.57      0.06      0.11        64\n",
            "\n",
            "   micro avg       0.70      0.54      0.61      2419\n",
            "   macro avg       0.34      0.18      0.19      2419\n",
            "weighted avg       0.60      0.54      0.52      2419\n",
            " samples avg       0.70      0.55      0.60      2419\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-QGBUgCLzPE"
      },
      "source": [
        "## Linear SVC hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAh9wdhKpp1h"
      },
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def callback(study, trial):\n",
        "    if study.best_trial.number == trial.number:\n",
        "        study.set_user_attr(key='best_model', value=trial.user_attrs['model'])"
      ],
      "metadata": {
        "id": "XwgrmIMsHkUq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EHONgBRprGE",
        "outputId": "c93a70ca-aa9e-4cca-bfe6-80d92befeb9b"
      },
      "source": [
        "def linearsvc_objective(trial):\n",
        "    params = dict(\n",
        "        C=trial.suggest_float('C', 1e-9, 1e2, log=True),\n",
        "        class_weight=trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "        loss=trial.suggest_categorical('loss', ['hinge', 'squared_hinge']),\n",
        "        max_iter=2000,\n",
        "        random_state=5\n",
        "    )\n",
        "\n",
        "    clf = MOC(LinearSVC(**params))\n",
        "    clf.fit(xtrain_basef, ytrain_b)\n",
        "    trial.set_user_attr(key=\"model\", value=clf)\n",
        "    \n",
        "    y_pred = clf.predict(xdev_basef)\n",
        "    return quick_f1(ydev_ml, y_pred)\n",
        "\n",
        "sampler = TPESampler(seed=22)\n",
        "linearsvc_study = optuna.create_study(sampler=sampler, direction='maximize')\n",
        "linearsvc_study.optimize(linearsvc_objective, n_trials=50, callbacks=[callback])\n",
        "\n",
        "\n",
        "clf2 = linearsvc_study.user_attrs['best_model']\n",
        "\n",
        "print(evaluate(clf2, xtest_basef, ytest_b))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf2.predict(xtrain_basef)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf2.predict(xdev_basef)))\n",
        "print('test: ', quick_f1(ytest_ml , clf2.predict(xtest_basef)))\n",
        "\n",
        "print(clf2.estimators_[0].get_params())\n",
        "print(linearsvc_study.best_params)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.21      0.27        28\n",
            "           1       0.55      0.59      0.57       175\n",
            "           2       0.51      0.55      0.53       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.59      0.30      0.40        43\n",
            "           5       0.86      0.98      0.91       403\n",
            "           6       0.50      0.06      0.11        16\n",
            "           7       0.00      0.00      0.00        53\n",
            "           8       0.73      0.99      0.84       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.00      0.00      0.00        45\n",
            "          11       0.00      0.00      0.00        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.82      0.17      0.28        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.75      0.15      0.24        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.40      0.05      0.08        44\n",
            "          21       1.00      0.08      0.14        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.53      0.74      0.61       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       1.00      0.02      0.03        59\n",
            "          27       0.50      0.16      0.24        25\n",
            "          28       1.00      0.14      0.24        22\n",
            "          29       0.64      0.67      0.65       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.14      0.06      0.09        48\n",
            "          32       0.86      0.76      0.80       181\n",
            "          33       0.67      0.25      0.36        16\n",
            "          34       0.86      0.19      0.31        99\n",
            "          35       0.64      0.11      0.19        64\n",
            "\n",
            "   micro avg       0.69      0.56      0.62      2419\n",
            "   macro avg       0.39      0.20      0.22      2419\n",
            "weighted avg       0.61      0.56      0.53      2419\n",
            " samples avg       0.69      0.57      0.61      2419\n",
            "\n",
            "train: 0.9619\n",
            "dev:   0.6683\n",
            "test:  0.6157\n",
            "{'C': 0.15640579894830894, 'class_weight': 'balanced', 'dual': True, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 2000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': 5, 'tol': 0.0001, 'verbose': 0}\n",
            "{'C': 0.15640579894830894, 'class_weight': 'balanced', 'loss': 'squared_hinge'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruWhrvluNSPT"
      },
      "source": [
        "## Non-Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQB-5XHKwgbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba1b0be-78da-4649-da77-9816f84afb89"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "def svc_objective(trial):\n",
        "    params = dict(\n",
        "        class_weight=trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "        kernel=trial.suggest_categorical('kernel', ['poly', 'rbf', 'sigmoid']),\n",
        "        gamma=trial.suggest_categorical('gamma', ['auto', 'scale']),\n",
        "        max_iter=3000,\n",
        "        random_state=5\n",
        "    )\n",
        "\n",
        "    clf = MOC(SVC(**params))\n",
        "    clf.fit(xtrain_basef, ytrain_b)\n",
        "    trial.set_user_attr(key=\"model\", value=clf)\n",
        "\n",
        "    y_pred = clf.predict(xdev_basef)\n",
        "    return quick_f1(ydev_ml, y_pred)\n",
        "\n",
        "sampler = TPESampler(seed=22)\n",
        "svc_study = optuna.create_study(direction='maximize')\n",
        "svc_study.optimize(svc_objective, n_trials=10, callbacks=[callback])\n",
        "\n",
        "\n",
        "clf3 = svc_study.user_attrs['best_model']\n",
        "\n",
        "print(evaluate(clf3, xtest_basef, ytest_b))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf3.predict(xtrain_basef)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf3.predict(xdev_basef)))\n",
        "print('test: ', quick_f1(ytest_ml , clf3.predict(xtest_basef)))\n",
        "\n",
        "print(clf3.estimators_[0].get_params())\n",
        "print(svc_study.best_params)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-12-23 09:16:24,645]\u001b[0m A new study created in memory with name: no-name-56f678e4-01c9-4ac2-84db-be36052bb17b\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "\u001b[32m[I 2021-12-23 09:19:16,461]\u001b[0m Trial 0 finished with value: 0.6474 and parameters: {'class_weight': 'balanced', 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:19:57,626]\u001b[0m Trial 1 finished with value: 0.4855 and parameters: {'class_weight': None, 'kernel': 'poly', 'gamma': 'auto'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "\u001b[32m[I 2021-12-23 09:23:53,422]\u001b[0m Trial 2 finished with value: 0.5013 and parameters: {'class_weight': None, 'kernel': 'poly', 'gamma': 'scale'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:24:39,835]\u001b[0m Trial 3 finished with value: 0.4855 and parameters: {'class_weight': None, 'kernel': 'poly', 'gamma': 'auto'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:25:29,462]\u001b[0m Trial 4 finished with value: 0.4855 and parameters: {'class_weight': None, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:26:18,163]\u001b[0m Trial 5 finished with value: 0.4855 and parameters: {'class_weight': None, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "\u001b[32m[I 2021-12-23 09:29:25,986]\u001b[0m Trial 6 finished with value: 0.6474 and parameters: {'class_weight': 'balanced', 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "\u001b[32m[I 2021-12-23 09:32:21,383]\u001b[0m Trial 7 finished with value: 0.6474 and parameters: {'class_weight': 'balanced', 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:33:03,075]\u001b[0m Trial 8 finished with value: 0.4855 and parameters: {'class_weight': None, 'kernel': 'poly', 'gamma': 'auto'}. Best is trial 0 with value: 0.6474.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:33:47,337]\u001b[0m Trial 9 finished with value: 0.4855 and parameters: {'class_weight': None, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 0 with value: 0.6474.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        28\n",
            "           1       0.52      0.63      0.57       175\n",
            "           2       0.51      0.45      0.48       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.80      0.09      0.17        43\n",
            "           5       0.82      1.00      0.90       403\n",
            "           6       0.00      0.00      0.00        16\n",
            "           7       0.00      0.00      0.00        53\n",
            "           8       0.71      0.98      0.82       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.00      0.00      0.00        45\n",
            "          11       0.00      0.00      0.00        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       1.00      0.07      0.14        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.67      0.05      0.09        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.00      0.00      0.00        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.47      0.85      0.60       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       0.00      0.00      0.00        59\n",
            "          27       0.00      0.00      0.00        25\n",
            "          28       0.00      0.00      0.00        22\n",
            "          29       0.61      0.66      0.63       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.00      0.00      0.00        48\n",
            "          32       0.82      0.75      0.79       181\n",
            "          33       0.00      0.00      0.00        16\n",
            "          34       0.75      0.03      0.06        99\n",
            "          35       0.00      0.00      0.00        64\n",
            "\n",
            "   micro avg       0.66      0.54      0.59      2419\n",
            "   macro avg       0.21      0.15      0.15      2419\n",
            "weighted avg       0.51      0.54      0.48      2419\n",
            " samples avg       0.66      0.55      0.59      2419\n",
            "\n",
            "train: 0.9909\n",
            "dev:   0.6474\n",
            "test:  0.5945\n",
            "{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 3000, 'probability': False, 'random_state': 5, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
            "{'class_weight': 'balanced', 'kernel': 'rbf', 'gamma': 'scale'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhF6HkLKNPSL"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3srrPw5Ay7mn",
        "outputId": "b61e6a84-27ca-4fb7-95be-abb23bdda4a5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def logistic_objective(trial):\n",
        "    params = dict(\n",
        "        class_weight=trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "        C=trial.suggest_float('C', 1e-5, 20),\n",
        "        random_state=5,\n",
        "        max_iter=200\n",
        "    )    \n",
        "\n",
        "    clf = MOC(LogisticRegression(**params))\n",
        "    clf.fit(xtrain_basef, ytrain_b)\n",
        "    trial.set_user_attr(key=\"model\", value=clf)\n",
        "\n",
        "    y_pred = clf.predict(xdev_basef)\n",
        "    return quick_f1(ydev_ml, y_pred)\n",
        "\n",
        "sampler = TPESampler(seed=22)\n",
        "logistic_study = optuna.create_study(sampler=sampler, direction='maximize')\n",
        "logistic_study.optimize(logistic_objective, n_trials=5, callbacks=[callback])\n",
        "\n",
        "\n",
        "clf4 = logistic_study.user_attrs['best_model']\n",
        "\n",
        "print(evaluate(clf4, xtest_basef, ytest_b))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf4.predict(xtrain_basef)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf4.predict(xdev_basef)))\n",
        "print('test:', quick_f1(ytest_ml  , clf4.predict(xtest_basef)))\n",
        "\n",
        "print(clf4.estimators_[0].get_params())\n",
        "print(logistic_study.best_params)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-12-23 09:36:01,441]\u001b[0m A new study created in memory with name: no-name-278b51b5-a4d0-42ef-a7c3-38dd87eec8ff\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:36:50,685]\u001b[0m Trial 0 finished with value: 0.659 and parameters: {'class_weight': None, 'C': 8.41076650090714}. Best is trial 0 with value: 0.659.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:37:39,030]\u001b[0m Trial 1 finished with value: 0.6671 and parameters: {'class_weight': 'balanced', 'C': 6.777285823434402}. Best is trial 1 with value: 0.6671.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:38:19,178]\u001b[0m Trial 2 finished with value: 0.6579 and parameters: {'class_weight': None, 'C': 4.408098128709346}. Best is trial 1 with value: 0.6671.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:39:15,237]\u001b[0m Trial 3 finished with value: 0.666 and parameters: {'class_weight': 'balanced', 'C': 11.224078321223027}. Best is trial 1 with value: 0.6671.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 09:39:59,739]\u001b[0m Trial 4 finished with value: 0.6646 and parameters: {'class_weight': 'balanced', 'C': 3.7822352140976876}. Best is trial 1 with value: 0.6671.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.21      0.28        28\n",
            "           1       0.55      0.59      0.57       175\n",
            "           2       0.50      0.66      0.57       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.52      0.53      0.53        43\n",
            "           5       0.89      0.93      0.91       403\n",
            "           6       0.50      0.06      0.11        16\n",
            "           7       0.27      0.06      0.09        53\n",
            "           8       0.75      0.96      0.84       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.44      0.09      0.15        45\n",
            "          11       0.33      0.07      0.12        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.60      0.44      0.51        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.47      0.54      0.50        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.40      0.08      0.14        24\n",
            "          20       0.27      0.41      0.33        44\n",
            "          21       1.00      0.08      0.14        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.50      0.85      0.63       205\n",
            "          24       0.17      0.11      0.13         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       0.80      0.07      0.12        59\n",
            "          27       0.43      0.24      0.31        25\n",
            "          28       0.57      0.18      0.28        22\n",
            "          29       0.56      0.77      0.65       128\n",
            "          30       0.43      0.12      0.18        26\n",
            "          31       0.11      0.06      0.08        48\n",
            "          32       0.75      0.83      0.79       181\n",
            "          33       0.67      0.38      0.48        16\n",
            "          34       0.70      0.35      0.47        99\n",
            "          35       0.51      0.36      0.42        64\n",
            "\n",
            "   micro avg       0.64      0.62      0.63      2419\n",
            "   macro avg       0.39      0.28      0.29      2419\n",
            "weighted avg       0.60      0.62      0.58      2419\n",
            " samples avg       0.65      0.63      0.62      2419\n",
            "\n",
            "train: 0.9967\n",
            "dev:   0.6671\n",
            "test: 0.6284\n",
            "{'C': 6.777285823434402, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 200, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 5, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
            "{'class_weight': 'balanced', 'C': 6.777285823434402}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGH2RTkEMkou"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4CLTr6W6lz7",
        "outputId": "3b0b9da7-f11d-4efd-a336-d6d0c7b332e7"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "\n",
        "def rfc_objective(trial):\n",
        "    params = dict(\n",
        "        bootstrap=trial.suggest_categorical('bootstrap', [True, False]),\n",
        "        max_depth=trial.suggest_categorical('max_depth', [None, 20, 40, 60, 80, 100]),\n",
        "        max_features=trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
        "        n_estimators=trial.suggest_int('n_estimators', 100, 200, step=20),\n",
        "        n_jobs=-1,\n",
        "        random_state=5\n",
        "    )\n",
        "\n",
        "    clf = RFC(**params)\n",
        "    clf.fit(xtrain_basef, ytrain_b)\n",
        "    trial.set_user_attr(key=\"model\", value=clf)\n",
        "    \n",
        "    y_pred = clf.predict(xdev_basef)\n",
        "    return quick_f1(ydev_ml, y_pred)\n",
        "\n",
        "\n",
        "sampler = TPESampler(seed=22)\n",
        "rfc_study = optuna.create_study(sampler=sampler, direction='maximize')\n",
        "rfc_study.optimize(rfc_objective, n_trials=20, callbacks=[callback])\n",
        "\n",
        "\n",
        "clf5 = rfc_study.user_attrs['best_model']\n",
        "\n",
        "print(evaluate(clf5, xtest_basef, ytest_b))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf5.predict(xtrain_basef)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf5.predict(xdev_basef)))\n",
        "print('test:', quick_f1(ytest_ml  , clf5.predict(xtest_basef)))\n",
        "\n",
        "print(clf5.get_params())\n",
        "print(rfc_study.best_params)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-12-23 08:53:14,331]\u001b[0m A new study created in memory with name: no-name-f86c6ef4-87da-4861-afe4-f9acb041ce0c\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:53:19,279]\u001b[0m Trial 0 finished with value: 0.5402 and parameters: {'bootstrap': False, 'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 160}. Best is trial 0 with value: 0.5402.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:53:27,844]\u001b[0m Trial 1 finished with value: 0.5834 and parameters: {'bootstrap': True, 'max_depth': 60, 'max_features': 'auto', 'n_estimators': 160}. Best is trial 1 with value: 0.5834.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:53:31,965]\u001b[0m Trial 2 finished with value: 0.5131 and parameters: {'bootstrap': False, 'max_depth': 60, 'max_features': 'log2', 'n_estimators': 180}. Best is trial 1 with value: 0.5834.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:53:33,640]\u001b[0m Trial 3 finished with value: 0.4861 and parameters: {'bootstrap': True, 'max_depth': 20, 'max_features': 'log2', 'n_estimators': 100}. Best is trial 1 with value: 0.5834.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:53:36,998]\u001b[0m Trial 4 finished with value: 0.5168 and parameters: {'bootstrap': False, 'max_depth': 80, 'max_features': 'log2', 'n_estimators': 120}. Best is trial 1 with value: 0.5834.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:53:47,551]\u001b[0m Trial 5 finished with value: 0.5831 and parameters: {'bootstrap': True, 'max_depth': 60, 'max_features': 'auto', 'n_estimators': 200}. Best is trial 1 with value: 0.5834.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:53:56,411]\u001b[0m Trial 6 finished with value: 0.5943 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 120}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:54:04,218]\u001b[0m Trial 7 finished with value: 0.5875 and parameters: {'bootstrap': True, 'max_depth': 100, 'max_features': 'sqrt', 'n_estimators': 120}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:54:12,259]\u001b[0m Trial 8 finished with value: 0.5909 and parameters: {'bootstrap': False, 'max_depth': 60, 'max_features': 'auto', 'n_estimators': 100}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:54:24,550]\u001b[0m Trial 9 finished with value: 0.588 and parameters: {'bootstrap': False, 'max_depth': 60, 'max_features': 'sqrt', 'n_estimators': 160}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:54:33,158]\u001b[0m Trial 10 finished with value: 0.5943 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 120}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:54:38,155]\u001b[0m Trial 11 finished with value: 0.5628 and parameters: {'bootstrap': True, 'max_depth': 40, 'max_features': 'auto', 'n_estimators': 120}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:54:47,929]\u001b[0m Trial 12 finished with value: 0.5939 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 140}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:54:57,718]\u001b[0m Trial 13 finished with value: 0.5939 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 140}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:55:06,315]\u001b[0m Trial 14 finished with value: 0.5943 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 120}. Best is trial 6 with value: 0.5943.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:55:13,471]\u001b[0m Trial 15 finished with value: 0.5953 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 100}. Best is trial 15 with value: 0.5953.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:55:20,462]\u001b[0m Trial 16 finished with value: 0.5953 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 100}. Best is trial 15 with value: 0.5953.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:55:27,727]\u001b[0m Trial 17 finished with value: 0.5953 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 100}. Best is trial 15 with value: 0.5953.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:55:34,820]\u001b[0m Trial 18 finished with value: 0.5953 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}. Best is trial 15 with value: 0.5953.\u001b[0m\n",
            "\u001b[32m[I 2021-12-23 08:55:44,497]\u001b[0m Trial 19 finished with value: 0.5939 and parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 140}. Best is trial 15 with value: 0.5953.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        28\n",
            "           1       0.42      0.06      0.11       175\n",
            "           2       0.53      0.27      0.35       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.00      0.00      0.00        43\n",
            "           5       0.81      1.00      0.89       403\n",
            "           6       0.00      0.00      0.00        16\n",
            "           7       0.00      0.00      0.00        53\n",
            "           8       0.70      0.98      0.82       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.00      0.00      0.00        45\n",
            "          11       0.00      0.00      0.00        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.00      0.00      0.00        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.00      0.00      0.00        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.00      0.00      0.00        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.57      0.02      0.04       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       0.00      0.00      0.00        59\n",
            "          27       0.00      0.00      0.00        25\n",
            "          28       0.00      0.00      0.00        22\n",
            "          29       0.73      0.45      0.55       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.00      0.00      0.00        48\n",
            "          32       0.90      0.21      0.34       181\n",
            "          33       0.00      0.00      0.00        16\n",
            "          34       0.00      0.00      0.00        99\n",
            "          35       0.00      0.00      0.00        64\n",
            "\n",
            "   micro avg       0.74      0.36      0.49      2419\n",
            "   macro avg       0.13      0.08      0.09      2419\n",
            "weighted avg       0.44      0.36      0.35      2419\n",
            " samples avg       0.73      0.38      0.49      2419\n",
            "\n",
            "train: 1.0\n",
            "dev:   0.5953\n",
            "test: 0.4854\n",
            "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 5, 'verbose': 0, 'warm_start': False}\n",
            "{'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c-DMHhxRmpD"
      },
      "source": [
        "## TwoStageSimple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQBOHruSRlKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f1d81e-e08b-4e59-ed4a-7a6ddaf1ce25"
      },
      "source": [
        "clf6 = TwoStageSimple(MOC(LinearSVC(**clf2.estimators_[0].get_params())),\n",
        "                      MOC(LinearSVC(**clf2.estimators_[0].get_params())))\n",
        "clf6.fit(xtrain_basef, ytrain_a, ytrain_b)\n",
        "\n",
        "print(evaluate(clf6, xtest_basef, ytest_b))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf6.predict(xtrain_basef)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf6.predict(xdev_basef)))\n",
        "print('test:', quick_f1(ytest_ml  , clf6.predict(xtest_basef)))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.21      0.29        28\n",
            "           1       0.52      0.65      0.57       175\n",
            "           2       0.46      0.58      0.51       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.86      0.14      0.24        43\n",
            "           5       0.85      0.94      0.89       403\n",
            "           6       0.00      0.00      0.00        16\n",
            "           7       0.00      0.00      0.00        53\n",
            "           8       0.72      0.97      0.83       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.53      0.22      0.31        45\n",
            "          11       0.19      0.18      0.19        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.54      0.54      0.54        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.43      0.61      0.51        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.16      0.57      0.25        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.47      0.89      0.61       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       0.33      0.24      0.28        59\n",
            "          27       1.00      0.08      0.15        25\n",
            "          28       0.50      0.05      0.08        22\n",
            "          29       0.44      0.84      0.58       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.18      0.17      0.17        48\n",
            "          32       0.62      0.87      0.72       181\n",
            "          33       0.50      0.12      0.20        16\n",
            "          34       0.50      0.41      0.45        99\n",
            "          35       0.34      0.50      0.40        64\n",
            "\n",
            "   micro avg       0.56      0.64      0.59      2419\n",
            "   macro avg       0.29      0.27      0.24      2419\n",
            "weighted avg       0.53      0.64      0.55      2419\n",
            " samples avg       0.58      0.65      0.60      2419\n",
            "\n",
            "train: 0.9549\n",
            "dev:   0.6334\n",
            "test: 0.5946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf7 = TwoStageSimple(MOC(SVC(**clf3.estimators_[0].get_params())),\n",
        "                      MOC(SVC(**clf3.estimators_[0].get_params())))\n",
        "clf7.fit(xtrain_basef, ytrain_a, ytrain_b)\n",
        "\n",
        "print(evaluate(clf7, xtest_basef, ytest_b))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf7.predict(xtrain_basef)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf7.predict(xdev_basef)))\n",
        "print('test: ', quick_f1(ytest_ml , clf7.predict(xtest_basef)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0UjbWQwh6AJ",
        "outputId": "aaedf029-bd9e-425e-b04f-fef0ed644a9e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning:\n",
            "\n",
            "Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.04      0.05        28\n",
            "           1       0.39      0.48      0.43       175\n",
            "           2       0.33      0.44      0.38       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.20      0.14      0.16        43\n",
            "           5       0.82      0.95      0.88       403\n",
            "           6       0.00      0.00      0.00        16\n",
            "           7       0.33      0.04      0.07        53\n",
            "           8       0.72      0.97      0.83       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       1.00      0.02      0.04        45\n",
            "          11       0.00      0.00      0.00        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       1.00      0.11      0.20        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.67      0.05      0.09        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.00      0.00      0.00        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.52      0.79      0.63       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       1.00      0.02      0.03        59\n",
            "          27       1.00      0.04      0.08        25\n",
            "          28       0.00      0.00      0.00        22\n",
            "          29       0.54      0.70      0.61       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.17      0.02      0.04        48\n",
            "          32       0.72      0.79      0.75       181\n",
            "          33       0.50      0.06      0.11        16\n",
            "          34       0.71      0.15      0.25        99\n",
            "          35       0.33      0.06      0.11        64\n",
            "\n",
            "   micro avg       0.62      0.53      0.57      2419\n",
            "   macro avg       0.31      0.16      0.16      2419\n",
            "weighted avg       0.55      0.53      0.48      2419\n",
            " samples avg       0.62      0.54      0.57      2419\n",
            "\n",
            "train: 0.9693\n",
            "dev:   0.6357\n",
            "test: 0.5707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKmFUKKcT6DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49a6cc9-d693-4337-a76a-16e9a385bbb3"
      },
      "source": [
        "clf8 = TwoStageSimple(MOC(LogisticRegression(**clf4.estimators_[0].get_params())),\n",
        "                      MOC(LogisticRegression(**clf4.estimators_[0].get_params())))\n",
        "clf8.fit(xtrain_basef, ytrain_a, ytrain_b)\n",
        "\n",
        "print(evaluate(clf8, xtest_basef, ytest_b))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf8.predict(xtrain_basef)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf8.predict(xdev_basef)))\n",
        "print('test:', quick_f1(ytest_ml  , clf8.predict(xtest_basef)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.25      0.30        28\n",
            "           1       0.52      0.58      0.55       175\n",
            "           2       0.47      0.62      0.54       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.48      0.30      0.37        43\n",
            "           5       0.87      0.94      0.90       403\n",
            "           6       1.00      0.06      0.12        16\n",
            "           7       0.20      0.06      0.09        53\n",
            "           8       0.73      0.95      0.83       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.42      0.18      0.25        45\n",
            "          11       0.22      0.14      0.17        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.51      0.39      0.44        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.50      0.61      0.55        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.24      0.17      0.20        24\n",
            "          20       0.20      0.48      0.29        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.49      0.85      0.62       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.43      0.05      0.09        62\n",
            "          26       0.42      0.17      0.24        59\n",
            "          27       0.50      0.16      0.24        25\n",
            "          28       0.44      0.18      0.26        22\n",
            "          29       0.50      0.83      0.62       128\n",
            "          30       0.22      0.08      0.11        26\n",
            "          31       0.15      0.15      0.15        48\n",
            "          32       0.67      0.82      0.74       181\n",
            "          33       0.27      0.25      0.26        16\n",
            "          34       0.52      0.36      0.43        99\n",
            "          35       0.43      0.50      0.46        64\n",
            "\n",
            "   micro avg       0.58      0.63      0.60      2419\n",
            "   macro avg       0.33      0.28      0.27      2419\n",
            "weighted avg       0.56      0.63      0.57      2419\n",
            " samples avg       0.60      0.63      0.60      2419\n",
            "\n",
            "train: 0.9963\n",
            "dev:   0.6511\n",
            "test: 0.6012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oec85dcXTm-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d615b253-6e86-4901-bac6-92ba8d3ac966"
      },
      "source": [
        "clf9 = TwoStageSimple(RFC(**clf5.get_params()),\n",
        "                      RFC(**clf5.get_params()))\n",
        "clf9.fit(xtrain_basef, ytrain_a, ytrain_b)\n",
        "\n",
        "print(evaluate(clf9, xtest_basef, ytest_b))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf9.predict(xtrain_basef)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf9.predict(xdev_basef)))\n",
        "print('test:', quick_f1(ytest_ml  , clf9.predict(xtest_basef)))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        28\n",
            "           1       0.56      0.14      0.23       175\n",
            "           2       0.46      0.52      0.49       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.00      0.00      0.00        43\n",
            "           5       0.81      1.00      0.89       403\n",
            "           6       0.00      0.00      0.00        16\n",
            "           7       0.00      0.00      0.00        53\n",
            "           8       0.69      0.99      0.82       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.00      0.00      0.00        45\n",
            "          11       0.00      0.00      0.00        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.00      0.00      0.00        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.00      0.00      0.00        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.00      0.00      0.00        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.25      0.00      0.01       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.00      0.00      0.00        62\n",
            "          26       0.00      0.00      0.00        59\n",
            "          27       0.00      0.00      0.00        25\n",
            "          28       0.00      0.00      0.00        22\n",
            "          29       0.68      0.49      0.57       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.00      0.00      0.00        48\n",
            "          32       0.90      0.36      0.52       181\n",
            "          33       0.00      0.00      0.00        16\n",
            "          34       0.00      0.00      0.00        99\n",
            "          35       0.00      0.00      0.00        64\n",
            "\n",
            "   micro avg       0.72      0.40      0.51      2419\n",
            "   macro avg       0.12      0.10      0.10      2419\n",
            "weighted avg       0.42      0.40      0.37      2419\n",
            " samples avg       0.71      0.42      0.51      2419\n",
            "\n",
            "train: 1.0\n",
            "dev:   0.61\n",
            "test: 0.5092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GZex0qHPgVW"
      },
      "source": [
        "## TwoStageAdvanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xa3I_YkPm7P",
        "outputId": "5c0e9742-4b4b-4d80-c86f-327a16bfbae0"
      },
      "source": [
        "clf10 = TwoStageAdvanced(MOC(LinearSVC(**clf2.estimators_[0].get_params())),\n",
        "                         MOC(LinearSVC(**clf2.estimators_[0].get_params())))\n",
        "clf10.fit(xtrain_a, xtrain_b, ytrain_a, ytrain_b)\n",
        "\n",
        "yb_true  = mo2ml(ytest_b)\n",
        "\n",
        "yb_pred  = mo2df(clf10.predict(xtest_a, xtest_b))\n",
        "yb_pred  = mo2ml(yb_pred)\n",
        "\n",
        "print(classification_report(yb_true, yb_pred, zero_division=0))\n",
        "\n",
        "print('train:', quick_f1(ytrain_ml, clf10.predict(xtrain_a, xtrain_b)))\n",
        "print('dev:  ', quick_f1(ydev_ml  , clf10.predict(xdev_a, xdev_b)))\n",
        "print('test:', quick_f1(ytest_ml  , clf10.predict(xtest_a, xtest_b)))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.14      0.20        28\n",
            "           1       0.52      0.39      0.44       175\n",
            "           2       0.44      0.56      0.49       128\n",
            "           3       0.00      0.00      0.00        11\n",
            "           4       0.60      0.21      0.31        43\n",
            "           5       0.87      0.95      0.91       403\n",
            "           6       0.00      0.00      0.00        16\n",
            "           7       0.00      0.00      0.00        53\n",
            "           8       0.74      0.88      0.81       334\n",
            "           9       0.00      0.00      0.00         3\n",
            "          10       0.50      0.07      0.12        45\n",
            "          11       0.19      0.18      0.18        28\n",
            "          12       0.00      0.00      0.00         6\n",
            "          13       0.00      0.00      0.00        11\n",
            "          14       0.57      0.48      0.52        54\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.37      0.34      0.35        41\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00        24\n",
            "          20       0.18      0.20      0.19        44\n",
            "          21       0.00      0.00      0.00        13\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.53      0.73      0.62       205\n",
            "          24       0.00      0.00      0.00         9\n",
            "          25       0.50      0.02      0.03        62\n",
            "          26       0.32      0.19      0.24        59\n",
            "          27       0.57      0.16      0.25        25\n",
            "          28       0.67      0.09      0.16        22\n",
            "          29       0.54      0.72      0.62       128\n",
            "          30       0.00      0.00      0.00        26\n",
            "          31       0.22      0.04      0.07        48\n",
            "          32       0.71      0.76      0.73       181\n",
            "          33       0.50      0.19      0.27        16\n",
            "          34       0.57      0.39      0.47        99\n",
            "          35       0.38      0.41      0.39        64\n",
            "\n",
            "   micro avg       0.62      0.56      0.59      2419\n",
            "   macro avg       0.30      0.23      0.23      2419\n",
            "weighted avg       0.56      0.56      0.54      2419\n",
            " samples avg       0.63      0.57      0.58      2419\n",
            "\n",
            "train: 0.9973\n",
            "dev:   0.6362\n",
            "test: 0.5899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export model"
      ],
      "metadata": {
        "id": "4O6OZnEc1xHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "import joblib\n",
        "\n",
        "pipe = make_pipeline(TextCleaner(), vectorizer, clf4)\n",
        "joblib.dump(pipe, 'pipe.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-YNmWUJ1y2R",
        "outputId": "ccb61a87-521c-426b-a7ed-5a7ef4091bd6"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pipe.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mLxD33gt3tH"
      },
      "source": [
        "# Draft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgLxF_zP7l26"
      },
      "source": [
        "## Unicode normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vOYbrYIAFjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54002ccb-3a51-48c8-a1a7-39240fc05d6e"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "dựng_sẵn = '\\u1EA0'\n",
        "tổ_hợp   = '\\u0041\\u0323'\n",
        "\n",
        "print(dựng_sẵn, tổ_hợp)\n",
        "print(dựng_sẵn == tổ_hợp)\n",
        "\n",
        "print('-'*10)\n",
        "\n",
        "dựng_sẵn = unicodedata.normalize('NFC', dựng_sẵn)\n",
        "tổ_hợp   = unicodedata.normalize('NFC', tổ_hợp)\n",
        "\n",
        "print(dựng_sẵn, tổ_hợp)\n",
        "print(dựng_sẵn == tổ_hợp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ạ Ạ\n",
            "False\n",
            "----------\n",
            "Ạ Ạ\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgNHI9f9B4d_"
      },
      "source": [
        "## Test TextCleaner class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W__UH-mI8isz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9ee6be-eb59-4b93-e3fa-31664daf797d"
      },
      "source": [
        "texts = ['K khí trong lành. đồ ăn hong ngon, thức uống  K tồi; 🥙🌮',\n",
        "         'khung cảnh xinh đẹp',\n",
        "         'khuyến mãi cực sốc giả chỉ 1000 đồng',\n",
        "         '200k quá mắc',\n",
        "         'món ăn này mắc quá tới 200k lận. ngày 23/3/2000 😴',\n",
        "         'mua 100.000vnd',\n",
        "         'bán 1,000,000 d. 5 cái bành xèo tốn 500k',\n",
        "         'bán 1.000.000 d. 5 cái bành xèo tốn 500k %^^4',\n",
        "         'món ăn này có giá 10 lít',\n",
        "         'món ăn này tận 100 nghìn đồng',\n",
        "         'bán 1.000đ',\n",
        "         'quán này có giá trung bình từ 100k-200k 😛',\n",
        "         'quán này có giá trung bình từ 100-200k 😫',\n",
        "         '#mắc #food',\n",
        "         'bàn ghế sạch đẹp, thái độ nhân viên ok#restaurant 😍',\n",
        "         '#tiktok ở nhà vẫn vui',\n",
        "         '# birthday ngày mai có tiệc ^^',\n",
        "         'aslkdhlakd#tiktok#learn asljdalskjd',\n",
        "         '#tiktok   #learn',\n",
        "         '#hastag alskjdlasjd #hastag asdsadas #hastag 😁',\n",
        "         '#123&456',\n",
        "         '#!?@!']\n",
        "\n",
        "cleaner = TextCleaner()\n",
        "for t in cleaner.fit_transform(texts):\n",
        "    print(t.strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "không khí trong lành đồ ăn hong ngon thức uống  không tồi\n",
            "khung cảnh xinh đẹp\n",
            "khuyến mãi cực sốc giả chỉ giátiền\n",
            "giátiền quá mắc\n",
            "món ăn này mắc quá tới giátiền lận ngày 2332000\n",
            "mua giátiền\n",
            "bán giátiền 5 cái bành xèo tốn giátiền\n",
            "bán giátiền 5 cái bành xèo tốn giátiền 4\n",
            "món ăn này có giá 10 lít\n",
            "món ăn này tận giátiền\n",
            "bán giátiền\n",
            "quán này có giá trung bình từ giátiềngiátiền\n",
            "quán này có giá trung bình từ 100giátiền\n",
            "hashtag hashtag\n",
            "bàn ghế sạch đẹp thái độ nhân viên okhashtag\n",
            "hashtag ở nhà vẫn vui\n",
            "birthday ngày mai có tiệc\n",
            "aslkdhlakdhashtag asljdalskjd\n",
            "hashtag   hashtag\n",
            "hashtag alskjdlasjd hashtag asdsadas hashtag\n",
            "hashtag\n",
            "hashtag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision-Recall-F1"
      ],
      "metadata": {
        "id": "U2L-qE_-EbEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp1 = ydev_ml\n",
        "tmp2 = mo2ml(mo2df(clf0.predict(Xdev)))\n",
        "print(precision_score(tmp1, tmp2, average='micro'))\n",
        "print(recall_score(tmp1, tmp2, average='micro'))\n",
        "print(f1_score(tmp1, tmp2, average='micro'))\n",
        "print()\n",
        "print(precision_score(tmp1, tmp2, average='macro', zero_division=0))\n",
        "print(recall_score(tmp1, tmp2, average='macro', zero_division=0))\n",
        "print(f1_score(tmp1, tmp2, average='macro', zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbBVKrptEYwE",
        "outputId": "555c033f-ba4a-4ff4-bb6a-183cf52fa6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6818042344277385\n",
            "0.645367412140575\n",
            "0.6630856460757983\n",
            "\n",
            "0.42682438155100083\n",
            "0.18689938230658137\n",
            "0.2106136408551627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_prec(target, predict, average='micro'):\n",
        "    if average == 'micro':\n",
        "        return (target & predict).values.sum() / predict.values.sum()\n",
        "    return ((target & predict).sum() / predict.sum()).fillna(0).mean()\n",
        "\n",
        "def my_reca(target, predict, average='micro'):\n",
        "    if average == 'micro':\n",
        "        return (target & predict).values.sum() / target.values.sum()\n",
        "    return ((target & predict).sum() / target.sum()).fillna(0).mean()\n",
        "\n",
        "def my_ftes(target, predict, average='micro'):\n",
        "    if average == 'macro':\n",
        "        p = ((target & predict).sum() / predict.sum()).fillna(0)\n",
        "        r = ((target & predict).sum() / target.sum()).fillna(0)\n",
        "        return (2*p*r / (p+r)).fillna(0).mean()\n",
        "    else:\n",
        "        p = my_prec(target, predict)\n",
        "        r = my_reca(target, predict)\n",
        "        return 2*p*r / (p+r)\n",
        "\n",
        "print(my_prec(tmp1, tmp2))\n",
        "print(my_reca(tmp1, tmp2))\n",
        "print(my_ftes(tmp1, tmp2))\n",
        "print()\n",
        "print(my_prec(tmp1, tmp2, average='macro'))\n",
        "print(my_reca(tmp1, tmp2, average='macro'))\n",
        "print(my_ftes(tmp1, tmp2, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i1HnRckEaJk",
        "outputId": "6514e54d-71d2-43ca-f04f-83939bf1fde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6818042344277385\n",
            "0.645367412140575\n",
            "0.6630856460757983\n",
            "\n",
            "0.42682438155100083\n",
            "0.18689938230658137\n",
            "0.21061364085516276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qv8rkpDxE2bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}