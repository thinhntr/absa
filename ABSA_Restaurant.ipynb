{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABSA-Restaurant.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqNjpT1v-qcU"
      },
      "source": [
        "# Implementation of the ABSA for Hotel system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzAgmYXf9p2x"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pU2aIqA_Kki",
        "outputId": "b7f3445e-3505-45a9-f9d4-987e1e5fb573"
      },
      "source": [
        "# Clone repo và không in ra progress (quite mode)\n",
        "!git clone -q https://github.com/thinhntr/CS221.M11.KHCL-Aspect-Based-Sentiment-Analysis\n",
        "\n",
        "# Kiểm tra đường dẫn hiện tại\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NgCU7RHWAG9",
        "outputId": "444690ad-5eba-4e4e-aedb-4a08219a2daf"
      },
      "source": [
        "# Hiện file/thư mục con trong đường dẫn hiện tại\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CS221.M11.KHCL-Aspect-Based-Sentiment-Analysis\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPy4D0UIEuo3"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZRA0oqt6lUs",
        "outputId": "c937afcb-185e-4501-9b8e-4ee5eb4a5139"
      },
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "repo_dir = Path('./CS221.M11.KHCL-Aspect-Based-Sentiment-Analysis')\n",
        "data_dir = repo_dir / 'data' / 'csv'\n",
        "\n",
        "train_fp = data_dir / 'train.csv'\n",
        "dev_fp   = data_dir / 'dev.csv'\n",
        "test_fp  = data_dir / 'test.csv'\n",
        "\n",
        "# Kiểm tra đường dẫn có tồn tại không\n",
        "assert train_fp.is_file()\n",
        "assert dev_fp.is_file()\n",
        "assert test_fp.is_file()\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_fp)\n",
        "dev_df = pd.read_csv(dev_fp)\n",
        "test_df = pd.read_csv(test_fp)\n",
        "\n",
        "len(train_df), len(dev_df), len(test_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2961, 1290, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR-f8l8JDo7c",
        "outputId": "015b50db-3cb6-4f3d-c9fd-aee760930685"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "enc = OrdinalEncoder()\n",
        "\n",
        "def get_X_yA_yAS(df, not_fitted=False):\n",
        "    X = df.review.values.copy()\n",
        "    y = df.drop('review', axis=1)\n",
        "    yA = y.notna().values\n",
        "    yAS = y.fillna('dne')\n",
        "    if not_fitted:\n",
        "        enc.fit(yAS)\n",
        "    yAS = enc.transform(yAS)\n",
        "    return X, yA, yAS\n",
        "\n",
        "\n",
        "X_train, yA_train, yAS_train = get_X_yA_yAS(train_df, not_fitted=True)\n",
        "X_dev, yA_dev, yAS_dev = get_X_yA_yAS(dev_df)\n",
        "X_test, yA_test, yAS_test = get_X_yA_yAS(test_df)\n",
        "\n",
        "print(X_train.shape, yA_train.shape, yAS_train.shape)\n",
        "print(X_dev.shape, yA_dev.shape, yAS_train.shape)\n",
        "print(X_test.shape, yA_test.shape, yAS_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2961,) (2961, 12) (2961, 12)\n",
            "(1290,) (1290, 12) (2961, 12)\n",
            "(500,) (500, 12) (2961, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JWEN3WPyR0z"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxhK8nMVzFLL"
      },
      "source": [
        "!pip install -q emoji\n",
        "\n",
        "import emoji\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "\n",
        "def remove_emoji(texts):\n",
        "    return np.array([emoji.get_emoji_regexp().sub('', text) for text in texts])\n",
        "\n",
        "\n",
        "emoji_remover = FunctionTransformer(remove_emoji)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBXH_TN8yTgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ecf56f-571b-4aa0-8fe3-7265786b8f9c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "input_preprocess = make_pipeline(emoji_remover, \n",
        "                                 TfidfVectorizer())\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    input_preprocess,\n",
        "    RandomForestClassifier()\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, yAS_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('pipeline',\n",
              "                 Pipeline(memory=None,\n",
              "                          steps=[('functiontransformer',\n",
              "                                  FunctionTransformer(accept_sparse=False,\n",
              "                                                      check_inverse=True,\n",
              "                                                      func=<function remove_emoji at 0x7fc303ecb050>,\n",
              "                                                      inv_kw_args=None,\n",
              "                                                      inverse_func=None,\n",
              "                                                      kw_args=None,\n",
              "                                                      validate=False)),\n",
              "                                 ('tfidfvectorizer',\n",
              "                                  TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                                  decode_error='strict',\n",
              "                                                  dtype...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd3kIRJh5-e9"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def multioutput_to_multilabel(y):\n",
        "    nrow = y.shape[0]\n",
        "    ncol = y.shape[1]\n",
        "    multilabel = np.zeros((nrow, 4 * ncol), dtype=np.bool)\n",
        "    for i in range(nrow):\n",
        "        for j in range(ncol):\n",
        "            pos = int(j * 4 + y[i, j])\n",
        "            multilabel[i, pos] = True\n",
        "    return multilabel\n",
        "\n",
        "\n",
        "def custom_f1_score(y_true, y_pred, **kwargs):\n",
        "    y_true = multioutput_to_multilabel(y_true)\n",
        "    y_pred = multioutput_to_multilabel(y_pred)\n",
        "    return f1_score(y_true, y_pred, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgP7vWafWHkU",
        "outputId": "becddc20-5a5b-4a07-9228-67e7becec2a7"
      },
      "source": [
        "y_pred = pipeline.predict(X_dev)\n",
        "custom_f1_score(yAS_dev, y_pred, average='samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8587855297157624"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVQjsuhIWyDQ",
        "outputId": "62598687-baec-47eb-b258-3d3e384a47d2"
      },
      "source": [
        "custom_f1_score(yAS_dev, y_pred, average='micro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8587855297157624"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTofWzYeXNNn",
        "outputId": "cc61fc23-4606-46f6-e182-ff3751ec5893"
      },
      "source": [
        "custom_f1_score(yAS_dev, y_pred, average='weighted', zero_division=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8233522651652272"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyzheg51XQUu",
        "outputId": "8e78704b-0051-4fd4-9d7e-7e7f25d9b793"
      },
      "source": [
        "custom_f1_score(yAS_dev, y_pred, average='macro', zero_division=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30169544051464864"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LA-cVF5-R59"
      },
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
        "\n",
        "# def preprocess(df):\n",
        "#     tokens = tokenizer(df.review.values.tolist(), \n",
        "#                        padding=\"max_length\", \n",
        "#                        truncation=True, \n",
        "#                        return_tensors=\"np\")\n",
        "\n",
        "#     X = tokens.input_ids\n",
        "#     y = df.drop('review', axis=1).notna().values\n",
        "#     return X, y\n",
        "\n",
        "# X_train, y_train = preprocess(train_df)\n",
        "# X_dev, y_dev = preprocess(dev_df)\n",
        "# X_test, y_test = preprocess(test_df)\n",
        "\n",
        "# X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmqq85da-GN7"
      },
      "source": [
        "# Draft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4LJMFax6FbP"
      },
      "source": [
        "```\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "aspects_url = 'https://raw.githubusercontent.com/thinhntr/CS221.M11.KHCL-Aspect-Based-Sentiment-Analysis/main/data/csv/aspects.json'\n",
        "aspects = json.loads(requests.get(aspects_url).text)\n",
        "\n",
        "def label_encoder(label):\n",
        "    y = [np.nan] * len(aspects)\n",
        "    ap_stm = re.findall('{(.+?), ([a-z]+)}', label)\n",
        "\n",
        "    for aspect, sentiment in ap_stm:\n",
        "        idx = aspects.index(aspect)\n",
        "        y[idx] = sentiment\n",
        "\n",
        "    return y\n",
        "\n",
        "def txt2df(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8-sig') as txt:\n",
        "        data = txt.read().split('\\n')\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df['review'] = [review for review in data[1::4]]\n",
        "    df[aspects] = [label_encoder(label) for label in data[2::4]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def label_decoder(encoded_label):\n",
        "    aps_stms = encoded_label[encoded_label.notna()]\n",
        "    \n",
        "    return ', '.join([f'{{{aspect}, {sentiment}}}' \n",
        "                      for aspect, sentiment in \n",
        "                      zip(aps_stms.index, aps_stms)])\n",
        "\n",
        "def csv2str(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    rows = []\n",
        "    for id, row in df.iterrows():\n",
        "        review = row[0]\n",
        "        labels = label_decoder(row[1:])\n",
        "        rows.extend((f'#{id+1}', review, labels, ''))\n",
        "    return '\\n'.join(rows)\n",
        "\n",
        "\n",
        "root_dir = Path('CS221.M11.KHCL-Aspect-Based-Sentiment-Analysis/data')\n",
        "\n",
        "train_txt_fp = root_dir/'original/1-VLSP2018-SA-Restaurant-train (7-3-2018).txt'\n",
        "dev_txt_fp = root_dir/'original/2-VLSP2018-SA-Restaurant-dev (7-3-2018).txt'\n",
        "test_txt_fp = root_dir/'original/3-VLSP2018-SA-Restaurant-test (8-3-2018).txt'\n",
        "\n",
        "train_csv_fp = root_dir/'csv/train.csv'\n",
        "dev_csv_fp = root_dir/'csv/dev.csv'\n",
        "test_csv_fp = root_dir/'csv/test.csv'\n",
        "\n",
        "assert train_txt_fp.is_file()\n",
        "assert dev_txt_fp.is_file()\n",
        "assert test_txt_fp.is_file()\n",
        "\n",
        "train_df = txt2df(train_fp)\n",
        "dev_df = txt2df(dev_fp)\n",
        "test_df = txt2df(test_fp)\n",
        "\n",
        "train_df.to_csv(train_csv_fp, index=False)\n",
        "dev_df.to_csv(dev_csv_fp, index=False)\n",
        "test_df.to_csv(test_csv_fp, index=False)\n",
        "\n",
        "print(csv2str(train_csv_fp))\n",
        "print(csv2str(dev_csv_fp))\n",
        "print(csv2str(test_csv_fp))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIPiv8cXc1if",
        "outputId": "1ff0cf55-1b3c-42aa-d7b3-430658d9ed59"
      },
      "source": [
        "!pip install -Uq scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkc-kWb2ccMk"
      },
      "source": [
        "from sklearn.feature_extraction.text import (CountVectorizer,\n",
        "                                             HashingVectorizer,\n",
        "                                             TfidfVectorizer,\n",
        "                                             TfidfTransformer)\n",
        "\n",
        "corpus = ['is First document.',\n",
        "          'is is second document']\n",
        "        #   'this is the first document?',\n",
        "        #   'and this is the third document']\n",
        "\n",
        "cvectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "X1 = np.asarray(cvectorizer.fit_transform(corpus).todense())\n",
        "\n",
        "X2 = np.asarray(TfidfTransformer().fit_transform(X1).todense())\n",
        "\n",
        "tvectorizer = TfidfVectorizer()\n",
        "X3 = tvectorizer.fit_transform(corpus).todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oORDLKZ_Pu14"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}